{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51278097-6ffa-4998-84ae-7661bd8165f1",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "Steps:\n",
    "    \n",
    "    - Detrending IIR Filter, Subsampling and Buterworth filter\n",
    "    \n",
    "    - Time domain features on each measurement (fault (number), severity (number), rpm, feature vector ...)\n",
    "    \n",
    "    - Save on disk - Load features from disk\n",
    "    \n",
    "    - (Later FD, TFD features)\n",
    "    \n",
    "    - PCM, DPCM, Features size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6032ab-ccf2-4105-8971-08d3eb62d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from itertools import pairwise\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.signal import welch, windows, find_peaks\n",
    "from scipy.interpolate import interp1d #, CubicSpline, PchipInterpolator\n",
    "from scipy.fft import rfft\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import mafaulda as src\n",
    "\n",
    "MAFAULDA_PATH = '../../datasets/MAFAULDA.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfaa4b8-c91c-4273-8c84-ce85c268e023",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- DC Removal\n",
    "- ANC - Noise removal\n",
    "- Downsampling + LowPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64666bd1-1077-4354-9d33-2c1e298b92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/how-to-filter-noise-with-a-low-pass-filter-python-885223e5e9b7\n",
    "# Low-pass filter 10 kHz (Butterworth)\n",
    "\n",
    "# Downsampling factor: 50 kHz / 10 kHz = 5\n",
    "# src.axis_spectrograms(misalign_sub)  (čistý subsampling vs. low-pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a55856-399f-464c-9b9f-f310ad8e107e",
   "metadata": {},
   "source": [
    "## Time domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97e398-57a9-45d7-b4c9-b5bf0cd6676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ZipFile(MAFAULDA_PATH)\n",
    "\n",
    "def parse_filename(filename):\n",
    "    fault = filename.split('/')\n",
    "    if fault[0].strip() in ('overhang', 'underhang'):\n",
    "        fault = f'{fault[0]}-{fault[1]}'\n",
    "        severity = fault[2]\n",
    "        seq = fault[3]\n",
    "    elif fault[0].strip() == 'normal':\n",
    "        fault, severity, seq = fault[0], '', fault[1]\n",
    "    else:\n",
    "        fault, severity, seq = fault\n",
    "\n",
    "    return fault, severity, seq\n",
    "\n",
    "def features_time_domain(zip_file, filename):\n",
    "    print(f'Processing: {filename}')\n",
    "    \n",
    "    ts = src.csv_import(zip_file, filename)\n",
    "    fault, severity, seq = parse_filename(filename)\n",
    "    rpm = ts['rpm'].mean()\n",
    "    columns = ['ax', 'ay', 'az', 'bx', 'by', 'bz', 'mic']\n",
    "    \n",
    "    feature_vector = [\n",
    "        ts[columns].mean().rename('mean'),\n",
    "        ts[columns].std().rename('std'),\n",
    "        ts[columns].apply(lambda x: skew(x)).rename('skew'),\n",
    "        ts[columns].apply(lambda x: kurtosis(x)).rename('kurt'),\n",
    "        ts[columns].apply(src.rms).rename('rms'),\n",
    "        ts[columns].apply(lambda x: np.max(x) - np.min(x)).rename('pp'),\n",
    "        ts[columns].apply(lambda x: np.max(np.absolute(x)) / src.rms(x)).rename('crest'),\n",
    "        ts[columns].apply(lambda x: np.max(np.absolute(x)) / np.mean(np.sqrt(np.absolute(x))) ** 2).rename('margin'),\n",
    "        ts[columns].apply(lambda x: np.max(np.absolute(x)) / np.mean(np.absolute(x))).rename('impulse'),\n",
    "        ts[columns].apply(lambda x: src.rms(x) / np.mean(np.absolute(x))).rename('shape'),\n",
    "        ts[columns].max().rename('max'),\n",
    "    ]\n",
    "    return (\n",
    "        pd.concat(feature_vector, axis=1)\n",
    "        .assign(fault=fault, severity=severity, seq=seq, rpm=rpm)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'axis'})\n",
    "    )\n",
    "    \n",
    "\n",
    "files = src.get_mafaulda_files(dataset) # [:10]  # TODO: all\n",
    "td_features = src.import_files(dataset, files, features_time_domain)\n",
    "td_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a64e7-fcec-4e22-b093-936c636498e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td_features.to_csv('td_features_no_filter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249944e8-a1a5-42d0-be3a-54a6de43d279",
   "metadata": {},
   "source": [
    "### Correlations in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b910881-6236-41d0-8a8e-8f775f2252b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('td_features_no_filter.csv')\n",
    "td_columns = ['mean', 'std', 'skew', 'kurt', 'rms', 'pp', 'crest', 'margin', 'impulse', 'shape']\n",
    "features[td_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a74625-2026-4ba1-b669-ce860b31d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(features[td_columns].corr(), cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a4a4e-172b-4f5b-96a1-6e9e3f34e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = features[td_columns].var().plot(kind='barh', xlabel='Variance', ylabel='Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cab23-bc92-48fb-a020-f76cda42cb09",
   "metadata": {},
   "source": [
    "## Frequency domain features\n",
    "Read also: https://librosa.org/doc/0.10.1/feature.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d2ae6-2151-4bec-9683-d91e17156e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERLAP = 0.5\n",
    "WINDOW_SIZES = (2**8, 2**10, 2**12, 2**14, 2**16)\n",
    "\n",
    "for w in WINDOW_SIZES:\n",
    "    src.resolution_calc(src.FS_HZ, w)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a17649-3fe5-4ca8-a399-7e5c4599b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_harmonics(f: np.array, Pxx: np.array) -> (np.array, np.array):\n",
    "    threshold = Pxx.mean() +  2*np.std(Pxx)\n",
    "    peaks, _ = find_peaks(Pxx)\n",
    "    f_harmonics = f[peaks]\n",
    "    y_harmonics = Pxx[peaks]\n",
    "\n",
    "    cond = y_harmonics >= threshold\n",
    "    loc_harmonics = peaks[cond]\n",
    "    f_harmonics = f_harmonics[cond]\n",
    "    return loc_harmonics, f_harmonics\n",
    "    \n",
    "\n",
    "def envelope_signal(f: np.array, Pxx: np.array) -> np.array:\n",
    "    peaks, _ = find_peaks(Pxx)\n",
    "    envelope = interp1d(f[peaks], Pxx[peaks], kind='quadratic', fill_value='extrapolate')\n",
    "    y_env = envelope(f)\n",
    "    y_env[y_env < 0] = 0\n",
    "    return y_env\n",
    "\n",
    "\n",
    "def spectral_transform(dataset: pd.DataFrame, axis: str, window: int) -> (np.array, np.array):\n",
    "    OVERLAP = 0.5\n",
    "    STEP = int(window * OVERLAP)\n",
    "    v = dataset[axis].to_numpy()\n",
    "    f, Pxx = welch(\n",
    "        v, fs=src.FS_HZ, window='hann',\n",
    "        nperseg=window, noverlap=STEP,\n",
    "        scaling='spectrum', average='mean', detrend='constant',\n",
    "        return_onesided=True\n",
    "    )\n",
    "    return f, Pxx\n",
    "\n",
    "\n",
    "def energy(Pxx: np.array) -> float:\n",
    "    return np.sum(Pxx**2)\n",
    "\n",
    "\n",
    "def spectral_roll_off_frequency(f: np.array, Pxx: np.array, percentage: float) -> float:\n",
    "    # Roll-off: Cumulative sum of energy in spectral bins and find index in f array\n",
    "    # 85% of total energy below this frequency\n",
    "    return f[np.argmax(np.cumsum(Pxx**2) >= percentage * energy(Pxx))]\n",
    "\n",
    "\n",
    "def temporal_variation(dataset: pd.DataFrame, axis: str, window: int) -> list:\n",
    "    # Temporal variation of succesive spectra (stationarity)\n",
    "    OVERLAP = 0.5\n",
    "    STEP = int(window * OVERLAP)\n",
    "    v = dataset[axis].to_numpy()\n",
    "    spectra = [\n",
    "        np.absolute(rfft(v[i:i+window] * windows.hann(window)))\n",
    "        for i in range(0, len(v) - window, STEP)\n",
    "    ]\n",
    "    # f = [i * (src.FS_HZ / window) for i in range(window // 2 + 1)]\n",
    "    fluxes = [\n",
    "        1 - np.corrcoef(psd1, psd2) for psd1, psd2 in pairwise(spectra)\n",
    "    ]\n",
    "    return fluxes\n",
    "\n",
    "\n",
    "def features_frequency_domain(zip_file: ZipFile, filename: str):\n",
    "    # Calculate FFT with Welch method in 5 different Hann window sizes\n",
    "    print(f'Processing: {filename}')\n",
    "    \n",
    "    ts = src.csv_import(zip_file, filename)\n",
    "    fault, severity, seq = parse_filename(filename)\n",
    "    rpm = ts['rpm'].mean()\n",
    "    columns = ['ax', 'ay', 'az', 'bx', 'by', 'bz', 'mic']\n",
    "    result = []\n",
    "    \n",
    "    for window in WINDOW_SIZES:\n",
    "        for col in columns:\n",
    "            f, Pxx = spectral_transform(ts, col, window)\n",
    "            \n",
    "            fluxes = temporal_variation(ts, col, window)\n",
    "            # TODO: harmonic part energy, noise part energy\n",
    "            noisiness = 0\n",
    "            inharmonicity = 0\n",
    "            # TODO: energies of each bin\n",
    "            envelope_spectrum = envelope_signal(f, Pxx)\n",
    "            loc_harmonics, _ = find_harmonics(f, Pxx)\n",
    "            # TODO: implement harmonics finder according to paper\n",
    "            \n",
    "            \n",
    "            result.append({\n",
    "                'fft_window_length': window,\n",
    "                'fault': fault,\n",
    "                'severity': severity,\n",
    "                'seq': seq,\n",
    "                'rpm': rpm,\n",
    "                'axis': col,\n",
    "                'centroid': np.average(f, weights=Pxx),\n",
    "                'std': np.std(Pxx),\n",
    "                'skew': skew(Pxx),\n",
    "                'kurt': kurtosis(Pxx),\n",
    "                'roll-off': spectral_roll_off_frequency(f, Pxx, 0.85),\n",
    "                'flux_mean': np.mean(fluxes),\n",
    "                'flux_std': np.std(fluxes),\n",
    "                'hdev': np.mean(envelope_spectrum[loc_harmonics] - Pxx[loc_harmonics]),\n",
    "                'noisiness': noisiness,\n",
    "                'inharmonicity': inharmonicity,\n",
    "                'energy': energy(Pxx),\n",
    "                'entropy': entropy(Pxx / np.sum(Pxx)),\n",
    "                'negentropy': -entropy((envelope_spectrum ** 2) / np.mean(envelope_spectrum ** 2))\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0a012-3758-4287-aa69-7adcb0a9b6da",
   "metadata": {},
   "source": [
    "### Extract frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377ee34-a7f0-4a7f-8948-ece68cf44ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ZipFile(MAFAULDA_PATH)\n",
    "files = src.get_mafaulda_files(dataset)[:10]  # TODO: all\n",
    "fd_features = src.import_files(dataset, files, features_frequency_domain)\n",
    "fd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8371067-e2ac-4c94-9e60-323b4b4175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_features.to_csv('fd_features_no_filter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020820b5-c044-4c47-90b5-5ef937d9f19b",
   "metadata": {},
   "source": [
    "### Experiment to find spectral envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0ece2-001e-4eb6-9dcd-6f208aabe324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate size of signal in PCM and DPCM (biggest number for differential), DPCM with len(hamming code) >= entropy in time domain\n",
    "\n",
    "# Experiment to find spectral envelope\n",
    "def plot_spectral_envelope(dataset, file, axis):\n",
    "    ts = src.csv_import(dataset, file)\n",
    "    f, Pxx = spectral_transform(ts, axis, 1024)\n",
    "    y_env = envelope_signal(f, Pxx)\n",
    "    # print(find_harmonics(f, Pxx))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "    ax.plot(f, Pxx)\n",
    "    ax.scatter(f[peaks], Pxx[peaks], color='red')\n",
    "    ax.plot(f, y_env)\n",
    "\n",
    "plot_spectral_envelope(ZipFile(MAFAULDA_PATH), 'vertical-misalignment/1.78mm/51.8144.csv', 'ax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d5528-7e05-4c0b-addc-e9b16e03531e",
   "metadata": {},
   "source": [
    "## Time-frequency domain features\n",
    "\n",
    "#### Features:\n",
    "- Energy\n",
    "- Entropy\n",
    "\n",
    "#### Transforms:\n",
    "- Discrete wavelet transform\n",
    "- Wavelet packet decompostion (Fejér-Korovkin wavelet)\n",
    "- Empirical wavelet transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f90a7-fcc2-4b39-b573-c4f80b8ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import ewtpy\n",
    "import warnings\n",
    "\n",
    "dataset = ZipFile(MAFAULDA_PATH)\n",
    "ts = src.csv_import(dataset, 'vertical-misalignment/1.78mm/51.8144.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb9f76-ce3d-4497-96ca-393d8c5cc95a",
   "metadata": {},
   "source": [
    "### Multilevel 1D Discrete Wavelet Transform\n",
    "https://www.mathworks.com/help/wavelet/gs/choose-a-wavelet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1969b1d-4b2f-4472-b107-b60dfbb0ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 'ax'\n",
    "wavelet = 'db8'\n",
    "result = pywt.wavedec(ts[axis], wavelet, mode='symmetric', level=3)\n",
    "print(len(ts[axis]))\n",
    "print([len(x) for x in result], '\\n', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2106-1698-4909-b9eb-139e68c39868",
   "metadata": {},
   "source": [
    "### Wavelet packet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64409099-9261-43f9-8aee-4f31edc5d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = pywt.WaveletPacket(data=ts['ax'], wavelet='db1', mode='symmetric')\n",
    "\n",
    "# Collect nodes based on frequency\n",
    "level = 3\n",
    "print([node.path for node in wp.get_level(level, 'freq')])\n",
    "print([len(node.data) for node in wp.get_level(level, 'freq')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5405f5-74ff-4e15-9bd3-9ba8f432ee1d",
   "metadata": {},
   "source": [
    "#### Energy in partitioned regions\n",
    "- store multiple levels to compare = {3, 6, 9}\n",
    "- multiple wavelets = {db1, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54049fb-8161-4c91-a695-84e962927279",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 3\n",
    "print('Energy')\n",
    "energies = [\n",
    "    energy(node.data) for node in wp.get_level(levels, 'freq')\n",
    "]\n",
    "total_energy = np.sum(energies)\n",
    "print(energies)\n",
    "print('Total Energy')\n",
    "print(total_energy)\n",
    "\n",
    "print('Energy ratio')\n",
    "energy_ratios = [\n",
    "    energy(node.data) / total_energy \n",
    "    for node in wp.get_level(levels, 'freq')\n",
    "]\n",
    "print(energy_ratios)\n",
    "\n",
    "print('Entropy')\n",
    "print([entropy(node.data**2) for node in wp.get_level(levels, 'freq')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc440087-cd75-4968-bc8a-8b7b17cd72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pywt.families()\n",
    "pywt.wavelist()\n",
    "w = pywt.Wavelet('db8')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f9952-2afb-41e5-b479-dbfa84664000",
   "metadata": {},
   "source": [
    "### Empirical Wavelet transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97236d3d-dd02-4abb-8345-deb22e989e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def ewt_transform(dataset: pd.DataFrame, axis: str, scales: int):\n",
    "    ewt, mfb, boundaries = ewtpy.EWT1D(\n",
    "        dataset[axis], N=scales,\n",
    "        log=0, detect='locmax', completion=0, \n",
    "        reg='average', lengthFilter=10, sigmaFilter=5\n",
    "    )\n",
    "    return ewt, mfb, boundaries\n",
    "    \n",
    "\n",
    "ewt, mfb, boundaries = ewt_transform(ts, 'ax', 3)\n",
    "#files = src.get_mafaulda_files(dataset)[:10]  # TODO: all\n",
    "#fd_features = src.import_files(dataset, files, features_frequency_domain)\n",
    "#fd_features\n",
    "ewt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
