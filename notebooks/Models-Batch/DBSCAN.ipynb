{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN\n",
    "- https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from kneed import KneeLocator\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from vibrodiagnostics import (\n",
    "    mafaulda,\n",
    "    discovery, \n",
    "    selection,\n",
    "    models\n",
    ")\n",
    "from vibrodiagnostics.models import (\n",
    "    fault_labeling, pipeline_v1, pipeline_v1_core, filter_out_metadata_columns\n",
    ")\n",
    "\n",
    "import re\n",
    "import os\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH_PREFIX = '../../datasets/'\n",
    "FEATURES_PATH =  os.path.join(PATH_PREFIX, 'features_data')\n",
    "\n",
    "TD_FD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_AND_FREQ_FEATURES_PATH)\n",
    "TD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_FEATURES_PATH)\n",
    "FD_FEATURES = os.path.join(FEATURES_PATH, selection.FREQ_FEATURES_PATH)\n",
    "\n",
    "domains = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "rpm_limit = [False, True]\n",
    "target = ['fault', 'anomaly_60', 'anomaly_90']\n",
    "placement = ['A', 'B']\n",
    "online = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor\n",
    "def get_features_list(domains):\n",
    "    features = []\n",
    "    for dname, dataset in domains.items():\n",
    "        names = pd.read_csv(dataset)\n",
    "        names = names.columns.str.extract(r'([a-z]{2})_([a-z\\_\\-]+)')[1].unique()\n",
    "        features.extend([f'{dname}_{col.strip(\"_\")}' for col in names if not pd.isnull(col)])\n",
    "\n",
    "    return features\n",
    "    \n",
    "def load_source(dataset: str, domain: str, row: dict):\n",
    "    faults = {\n",
    "        'A': {\n",
    "            'normal': 'normal',\n",
    "            'imbalance': 'imbalance',\n",
    "            'horizontal-misalignment': 'misalignment',\n",
    "            'vertical-misalignment': 'misalignment',\n",
    "            # 'underhang-outer_race': 'outer race fault',\n",
    "            # 'underhang-cage_fault': 'cage fault',\n",
    "            # 'underhang-ball_fault': 'ball fault'\n",
    "        },\n",
    "        'B': {\n",
    "            'normal': 'normal',\n",
    "            'imbalance': 'imbalance',\n",
    "            'horizontal-misalignment': 'misalignment',\n",
    "            'vertical-misalignment': 'misalignment',\n",
    "            # 'overhang-cage_fault': 'cage fault',\n",
    "            # 'overhang-ball_fault': 'ball fault',\n",
    "            # 'overhang-outer_race': 'outer race fault'\n",
    "        }\n",
    "    }\n",
    "    placements = {\n",
    "        'A': ['ax', 'ay', 'az'],\n",
    "        'B': ['bx', 'by', 'bz']\n",
    "    }\n",
    "    RPM = 2500\n",
    "    RPM_RANGE = 500\n",
    "    features = pd.read_csv(dataset).fillna(0)\n",
    "\n",
    "    # Choosing rpm range\n",
    "    if row['rpm_limit']:\n",
    "        features = features[features['rpm'].between(RPM - RPM_RANGE, RPM + RPM_RANGE, inclusive='both')]\n",
    "\n",
    "    # Labeling anomaly severity levels\n",
    "    target = re.search(r'([a-z]+)_?(\\d+)?', row['target'])\n",
    "    anomaly_severity = target.group(2) or '60'\n",
    "    anomaly_severity = float(anomaly_severity) / 100\n",
    "\n",
    "    # Choose measurement placement: A or B\n",
    "    place = row['placement']\n",
    "    axis = placements[place]\n",
    "    features = features[features['fault'].isin(tuple(faults[place]))]\n",
    "    features = models.fault_labeling(features, faults[place], anomaly_severity)\n",
    "\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]]\n",
    "\n",
    "    # Select predicted variable column\n",
    "    label = target.group(1)\n",
    "    Y = features[label].astype('category')\n",
    "\n",
    "    # Filter columns in feature domain with window size 2**14\n",
    "    if domain == 'spectral':\n",
    "        window_size = 2**14\n",
    "        X = X.loc[:,X.columns.str.endswith(f'_{window_size}')]\n",
    "        X.columns = X.columns.str.extract(r'(\\w+)_\\w+$')[0]\n",
    "\n",
    "    # Calculate feature magnitudes from 3D vector\n",
    "    feature_names = get_features_list({domain: dataset})\n",
    "    result = pd.DataFrame()\n",
    "    for name in feature_names:              \n",
    "        # Remove prefix: temporal, spectral\n",
    "        name = re.search(r'[a-z]+_([\\w\\_]+)', name).group(1)\n",
    "        vector_dims = [f'{dim}_{name}' for dim in axis]\n",
    "        result[name] = X[vector_dims].apply(np.linalg.norm, axis=1)\n",
    "    X = result\n",
    "\n",
    "    # Batch / Online hold-out (balance and event sequencing)\n",
    "    train_size = 0.8\n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='not majority', random_state=10)\n",
    "    X, Y = oversample.fit_resample(X, Y.to_numpy())\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    Y = pd.Series(Y)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, train_size=train_size, stratify=Y, random_state=10\n",
    "    )\n",
    "\n",
    "    ################ KEEP HERE ############################x\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[X_train.columns] = scaler.fit_transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "    ##############################################\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = 'temporal'\n",
    "dataset = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "config = {'rpm_limit': False, 'placement': 'A', 'domain': DOMAIN, 'target': 'anomaly_90'}\n",
    "X_train, X_test, y_train, y_test = load_source(dataset[DOMAIN], config['domain'], config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distances among points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_neighbors = 6\n",
    "neighbors = NearestNeighbors(n_neighbors=cnt_neighbors)\n",
    "neighbors.fit(X_train)\n",
    "distances, indices = neighbors.kneighbors(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plot distances among points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "distance_desc = sorted(distances[:, 1], reverse=True)\n",
    "ax.plot(list(range(1, len(distance_desc) + 1)), distance_desc)\n",
    "ax.set_xlabel('Number of points')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneedle = KneeLocator(range(1, len(distance_desc) + 1), distance_desc,\n",
    "                      S=1.0, curve='convex', direction='decreasing')\n",
    "kneedle.plot_knee_normalized()\n",
    "print(kneedle.elbow, kneedle.knee_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cuts_3d_cluster(X_train, cluster, title):\n",
    "    df = X_train.copy()\n",
    "    df['cluster'] = cluster\n",
    "    df['cluster'] = df['cluster'].astype('category')\n",
    "\n",
    "    categories = df['cluster'].cat.categories\n",
    "    colors = sb.color_palette('hls', len(categories))\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i, axes in enumerate(((0, 1), (0, 2), (1, 2))):\n",
    "        a, b = axes\n",
    "         \n",
    "        for label, color in zip(categories, colors):\n",
    "            rows = list(df[df['cluster'] == label].index)\n",
    "            x = df.loc[rows, df.columns[a]]\n",
    "            y = df.loc[rows, df.columns[b]]\n",
    "            ax[i].scatter(x, y, s=1, color=color, label=label)\n",
    "\n",
    "        ax[i].set_xlabel(df.columns[a])\n",
    "        ax[i].set_ylabel(df.columns[b])\n",
    "        ax[i].grid()\n",
    "        ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Range of values is MinMaxScaled in range (0, 1) - eps must be smaller than 1\n",
    "- Noisy samples are given the label -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = 'temporal'\n",
    "dataset = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "config = {'rpm_limit': False, 'placement': 'A', 'domain': DOMAIN, 'target': 'anomaly_90'}\n",
    "X_train, X_test, y_train, y_test = load_source(dataset[DOMAIN], config['domain'], config)\n",
    "\n",
    "fsel = ['shape', 'std', 'margin']\n",
    "X_train = X_train[fsel]\n",
    "X_test = X_test[fsel]\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train, 'Ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.05, min_samples=5, metric='l2')\n",
    "clustering.fit(X_train)\n",
    "y_train_labels = clustering.labels_\n",
    "y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train_labels, 'Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clustering(X_train, y_train_labels, X_test, y_predict):\n",
    "    # The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "    # The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters\n",
    "    print('Silhouette score:')\n",
    "    print('Train:', silhouette_score(X_train, y_train_labels, metric='euclidean'))\n",
    "    print('Test:', silhouette_score(X_test, y_predict, metric='euclidean'))\n",
    "\n",
    "    # Davies–Bouldin index: The minimum score is zero, with lower values indicating better clustering.\n",
    "    print('Davies-Bouldin index')\n",
    "    print('Train:', davies_bouldin_score(X_train, y_train_labels))\n",
    "    print('Test:', davies_bouldin_score(X_test, y_predict))\n",
    "\n",
    "    occurences = pd.DataFrame(\n",
    "        data=contingency_matrix(y_train, y_train_labels),\n",
    "        index=np.unique(y_train),\n",
    "        columns=np.unique(y_train_labels)\n",
    "    )\n",
    "    ax = sb.heatmap(occurences, cbar=True, cmap='BuGn', annot=True, fmt='d')\n",
    "\n",
    "\n",
    "evaluate_clustering(X_train, y_train_labels, X_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = 'spectral'\n",
    "dataset = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "config = {'rpm_limit': False, 'placement': 'A', 'domain': DOMAIN, 'target': 'anomaly_90'}\n",
    "X_train, X_test, y_train, y_test = load_source(dataset[DOMAIN], config['domain'], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.5, min_samples=5, metric='l2')\n",
    "clustering.fit(X_train)\n",
    "y_train_labels = clustering.labels_\n",
    "y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train_labels, 'Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_clustering(X_train, y_train_labels, X_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters for DBSCAN in supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_clustering_score(X, y, num_of_features, eps, min_samples):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    clusters = []\n",
    "\n",
    "    for train_index, test_index in crossvalid.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "        X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = pipeline_v1_core(\n",
    "            FSEL_METHOD, int(num_of_features), \n",
    "            X_train, y_train, X_test, y_test\n",
    "        )\n",
    "\n",
    "        clustering = DBSCAN(eps=eps, min_samples=int(min_samples), metric='l2')\n",
    "        clustering.fit(X_train)\n",
    "        y_train_labels = clustering.labels_\n",
    "        y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "        num_of_clusters = len(np.unique(y_train_labels))\n",
    "        clusters.append(num_of_clusters)\n",
    "        if num_of_clusters > 1:\n",
    "            train_scores.append(silhouette_score(X_train, y_train_labels, metric='euclidean'))\n",
    "            #test_scores.append(silhouette_score(X_test, y_predict, metric='euclidean'))\n",
    "\n",
    "    clusters = stats.mode(clusters).mode\n",
    "    train_scores = np.array(train_scores)\n",
    "    test_scores = np.array(test_scores)\n",
    "\n",
    "    clusters = np.mean(clusters)\n",
    "    train_score_mean = np.mean(train_scores)\n",
    "    train_score_std = np.std(train_scores)\n",
    "    #test_score_mean = np.mean(test_scores)\n",
    "    #test_score_std = np.std(test_scores)\n",
    "\n",
    "    return clusters, train_score_mean, train_score_std #test_score_mean, test_score_std\n",
    "\n",
    "\n",
    "\n",
    "DOMAIN = 'temporal'\n",
    "dataset = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "config = {'rpm_limit': False, 'placement': 'A', 'domain': DOMAIN, 'target': 'fault'}\n",
    "X, _, y, _ = load_source(dataset[DOMAIN], config['domain'], config)\n",
    "\n",
    "crossvalid = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "num_of_features = np.arange(1, len(X.columns) + 1)\n",
    "eps = np.linspace(0.05, 0.8, 8)\n",
    "min_samples = np.linspace(3, 8, 2)\n",
    "\n",
    "grid = np.array(np.meshgrid(num_of_features, eps, min_samples)).T.reshape(-1, 3)\n",
    "\n",
    "rows = []\n",
    "for f, e, s in tqdm(grid):\n",
    "    row = [f, e, s]\n",
    "    row.extend(cross_validate_clustering_score(X, y, f, e, s))\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "results = pd.DataFrame(rows, columns=[\n",
    "    'num_of_features', 'eps', 'min_samples', 'clusters', \n",
    "    'train_score_mean', 'train_score_std'\n",
    "]).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 best scored parameters with silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['clusters'] > 1].sort_values(by='train_score_mean', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
