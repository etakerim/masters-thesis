{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "import os\n",
    "from itertools import pairwise\n",
    "from typing import Callable, List, Tuple\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import find_peaks, butter, lfilter, windows, welch\n",
    "from scipy.fft import rfft\n",
    "from scipy.interpolate import interp1d\n",
    "from tsfel import feature_extraction as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../../datasets/'\n",
    "path_features = os.path.join(path_root, 'features')\n",
    "\n",
    "idx = 1        # Choose dataset [0, 1]\n",
    "datasets = ['MAFAULDA.zip', 'FluidPump.zip']\n",
    "names = [name.split('.')[0].lower() for name in datasets]\n",
    "parts = [1, 12]         # 5 second recordings (5/5, 60/5)\n",
    "\n",
    "opt = {\n",
    "    'name': names[idx],\n",
    "    'dataset': os.path.join(path_root, datasets[idx]),\n",
    "    'temporal_features': os.path.join(path_features, f'{names[idx]}_temporal.csv'),\n",
    "    'spectral_features': os.path.join(path_features, f'{names[idx]}_spectral.csv'),\n",
    "    'parts': parts[idx]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(Pxx: np.array) -> float:\n",
    "    return np.sum(Pxx**2)\n",
    "\n",
    "\n",
    "def negentropy(x: np.array) -> float:\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    return -entropy((x ** 2) / np.mean(x ** 2))\n",
    "\n",
    "\n",
    "def signal_to_noise(x: np.array) -> float:\n",
    "    # https://dsp.stackexchange.com/questions/76291/how-to-extract-noise-from-a-signal-in-order-to-get-both-noise-power-and-signal-p\n",
    "    # https://www.geeksforgeeks.org/signal-to-noise-ratio-formula/\n",
    "    # https://saturncloud.io/blog/calculating-signaltonoise-ratio-in-python-with-scipy-v11/\n",
    "    m = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    return np.where(sd == 0, 0, m / sd)\n",
    "\n",
    "\n",
    "def spectral_roll_off_frequency(f: np.array, Pxx: np.array, percentage: float) -> float:\n",
    "    \"\"\"Roll-off: Cumulative sum of energy in spectral bins and find index in f array\n",
    "    'percentage' % of total energy below this frequency\n",
    "    \"\"\"\n",
    "    return f[np.argmax(np.cumsum(Pxx**2) >= percentage * energy(Pxx))]\n",
    "\n",
    "\n",
    "def temporal_variation(dataset: pd.DataFrame, axis: str, window: int) -> list:\n",
    "    \"\"\"Temporal variation of succesive spectra (stationarity)\n",
    "    \"\"\"\n",
    "    overlap = 0.5\n",
    "    step = int(window * overlap)\n",
    "    v = dataset[axis].to_numpy()\n",
    "    spectra = [\n",
    "        np.absolute(rfft(v[i:i+window] * windows.hann(window)))\n",
    "        for i in range(0, len(v) - window, step)\n",
    "    ]\n",
    "    fluxes = [\n",
    "        1 - np.corrcoef(psd1, psd2) for psd1, psd2 in pairwise(spectra)\n",
    "    ]\n",
    "    return fluxes\n",
    "\n",
    "\n",
    "def envelope_signal(f: np.array, Pxx: np.array) -> np.array:\n",
    "    peaks, _ = find_peaks(Pxx)      # peaks = mms_peak_finder(Pxx)\n",
    "    try:\n",
    "        envelope = interp1d(f[peaks], Pxx[peaks], kind='quadratic', fill_value='extrapolate')\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "    y_env = envelope(f)\n",
    "    y_env[y_env < 0] = 0\n",
    "    return y_env\n",
    "\n",
    "\n",
    "def spectral_transform(dataset: pd.DataFrame, axis: str, window: int, fs: int) -> Tuple[np.array, np.array]:\n",
    "    overlap = 0.5\n",
    "    step = int(window * overlap)\n",
    "\n",
    "    v = dataset[axis].to_numpy()\n",
    "    f, pxx = welch(\n",
    "        v,\n",
    "        fs=fs,\n",
    "        window='hann',\n",
    "        nperseg=window,\n",
    "        noverlap=step,\n",
    "        scaling='spectrum',\n",
    "        average='mean',\n",
    "        detrend='constant',\n",
    "        return_onesided=True\n",
    "    )\n",
    "    return f, pxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features_calc(df: pd.DataFrame, col: str, fs: int, window: int) -> List[Tuple[str, pd.DataFrame]]:\n",
    "    x = df[col]\n",
    "    features = [\n",
    "        ('zerocross', ft.zero_cross(x) / len(x)),\n",
    "        ('pp', [ft.pk_pk_distance(x)]),\n",
    "        ('aac', np.mean(np.absolute(np.diff(x)))),\n",
    "        ('rms', [ft.rms(x)]),\n",
    "        ('skewness', [ft.skewness(x)]),\n",
    "        ('kurtosis', [ft.kurtosis(x)]),\n",
    "        ('shape', [ft.rms(x) / np.mean(np.absolute(x))]),\n",
    "        ('crest', [np.max(np.absolute(x)) / ft.rms(x)]),\n",
    "        ('impulse', [np.max(np.absolute(x)) / np.mean(np.absolute(x))]),\n",
    "        ('clearance', [np.max(np.absolute(x)) / (np.mean(np.sqrt(np.absolute(x))) ** 2)]),\n",
    "    ]\n",
    "    return [(f'{col}_{f[0]}', f[1]) for f in features]\n",
    "\n",
    "def frequency_features_calc(df: pd.DataFrame, col: str, fs: int, window: int) -> List[Tuple[str, pd.DataFrame]]:\n",
    "    f, pxx = spectral_transform(df, col, window, fs)\n",
    "    \n",
    "    fluxes = temporal_variation(df, col, window)\n",
    "    envelope_spectrum = envelope_signal(f, pxx)\n",
    "\n",
    "    features = [\n",
    "        ('centroid', [np.average(f, weights=pxx)]),\n",
    "        ('std', [ft.calc_std(pxx)]),\n",
    "        ('skewness', [ft.skewness(pxx)]),\n",
    "        ('kurtosis', [ft.kurtosis(pxx)]),\n",
    "        ('roll_on', [spectral_roll_off_frequency(f, pxx, 0.05)]),\n",
    "        ('roll_off', [spectral_roll_off_frequency(f, pxx, 0.85)]),\n",
    "        ('flux', [np.mean(fluxes)]),\n",
    "        ('noisiness', [signal_to_noise(pxx)]),\n",
    "        ('energy', [energy(pxx)]),\n",
    "        ('entropy', [entropy(pxx / np.sum(pxx))]),\n",
    "        ('negentropy', [negentropy(envelope_spectrum)])\n",
    "    ]\n",
    "    return [(f'{col}_{f[0]}', f[1]) for f in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from ZIP archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction (Generic)\n",
    "def list_files(dataset: ZipFile) -> List[str]:\n",
    "    filenames = [\n",
    "        f.filename\n",
    "        for f in dataset.infolist()\n",
    "        if f.filename.endswith(('.csv', '.tsv'))\n",
    "    ]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def load_files_split(dataset: ZipFile, func: Callable, parts: int, cores: int = 4):\n",
    "    pool = ThreadPool(processes=cores)\n",
    "    filenames = list_files(dataset)\n",
    "\n",
    "    return pd.concat([\n",
    "        pool.apply_async(func, (dataset, name, parts)).get()\n",
    "        for name in tqdm(filenames)\n",
    "    ])\n",
    "\n",
    "\n",
    "def split_dataframe(dataframe: pd.DataFrame, parts: int = None) -> List[pd.DataFrame]:\n",
    "    if parts is None:\n",
    "        return [dataframe]\n",
    "\n",
    "    step = len(dataframe) // parts\n",
    "    return [\n",
    "        dataframe.iloc[i:i+step].reset_index(drop=True)\n",
    "        for i in range(0, len(dataframe), step)\n",
    "        if len(dataframe.iloc[i:i + step]) == step\n",
    "    ]\n",
    "\n",
    "\n",
    "def detrending_filter(dataframes: List[pd.DataFrame], columns: List[str]) -> List[pd.DataFrame]:\n",
    "    for df in dataframes:\n",
    "        df[columns] = df[columns].apply(lambda x: x - x.mean())\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaFaulDa dataset signal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction mafaulda\n",
    "mafaulda_columns = ['ax', 'ay', 'az', 'bx', 'by', 'bz']\n",
    "mafaulda_all_columns = ['tachometer', 'ax', 'ay', 'az', 'bx', 'by', 'bz', 'mic']\n",
    "mafaulda_fs_hz = 50000\n",
    "mafaulda_spectral_window = 2**15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mafaulda_parse_filename(filename: str) -> Tuple[str, str, str]:\n",
    "    path = filename.split('/')\n",
    "\n",
    "    if path[0].strip() in ('overhang', 'underhang'):\n",
    "        fault = f'{path[0]}-{path[1]}'\n",
    "        severity = path[2]\n",
    "        seq = path[3]\n",
    "    \n",
    "    elif path[0].strip() == 'normal':\n",
    "        fault, severity, seq = path[0], '0', path[1]\n",
    "\n",
    "    else:\n",
    "        fault, severity, seq = path\n",
    "\n",
    "    return fault, severity, seq\n",
    "\n",
    "\n",
    "def rpm_calc(tachometer: pd.Series) -> float:\n",
    "    t = tachometer.index.to_numpy()\n",
    "    y = tachometer.to_numpy()\n",
    "    peaks, _ = find_peaks(y, prominence=3, width=50)\n",
    "    interval = np.diff(t[peaks]).mean()\n",
    "    return 60 / interval\n",
    "\n",
    "\n",
    "def mafaulda_csv_import(dataset: ZipFile, filename: str) -> pd.DataFrame:\n",
    "    ts = pd.read_csv(dataset.open(filename), names=mafaulda_all_columns)\n",
    "    T = 1 / mafaulda_fs_hz\n",
    "    ts = (\n",
    "        ts\n",
    "        .assign(t = lambda x: x.index * T)\n",
    "        .reset_index()\n",
    "        .assign(t = lambda x: x.index * T)\n",
    "        .set_index('t')\n",
    "        .assign(rpm = lambda x: rpm_calc(x.tachometer))\n",
    "    )\n",
    "    return ts.assign(key=filename)\n",
    "\n",
    "\n",
    "def mafaulda_lowpass_filter(\n",
    "        data: pd.Series,\n",
    "        cutoff: int = mafaulda_fs_hz // 5,\n",
    "        fs: int = mafaulda_fs_hz,\n",
    "        order: int = 5) -> pd.Series:\n",
    "    \n",
    "    b, a = butter(order, cutoff, fs=fs, btype='lowpass')\n",
    "    y = lfilter(b, a, data.to_numpy())\n",
    "    return pd.Series(data=y, index=data.index)\n",
    "\n",
    "\n",
    "def lowpass_filter_extract(dataframes: List[pd.DataFrame], columns: List[str]) -> List[pd.DataFrame]:\n",
    "    for df in dataframes:\n",
    "        df[columns] = df[columns].apply(mafaulda_lowpass_filter)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def mafaulda_features_by_domain(\n",
    "        features_calc: Callable,\n",
    "        dataset: ZipFile,\n",
    "        filename: str, \n",
    "        parts: int = None) -> pd.DataFrame:\n",
    "\n",
    "    # print(f'Processing: {filename}')\n",
    "    fs = mafaulda_fs_hz\n",
    "    columns = mafaulda_columns\n",
    "    window = mafaulda_spectral_window\n",
    "\n",
    "    ts = mafaulda_csv_import(dataset, filename)\n",
    "    fault, severity, seq = mafaulda_parse_filename(filename)\n",
    "\n",
    "    dataframe = split_dataframe(ts, parts)\n",
    "    dataframe = detrending_filter(dataframe, columns)\n",
    "    dataframe = lowpass_filter_extract(dataframe, columns)\n",
    "\n",
    "    result = []\n",
    "    for i, df in enumerate(dataframe):\n",
    "        fvector = [\n",
    "            ('fault', [fault]),\n",
    "            ('severity', [severity]),\n",
    "            ('seq', [f'{seq}.part.{i}']),\n",
    "            ('rpm', [df['rpm'].mean()])\n",
    "        ]\n",
    "        for col in columns:\n",
    "            fvector.extend(features_calc(df, col, fs, window))\n",
    "        result.append(pd.DataFrame(dict(fvector))) \n",
    "\n",
    "    return pd.concat(result).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fluid pumps dataset signal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluid pumps extraction\n",
    "fluidpump_columns = ['x', 'y', 'z']\n",
    "fluidpump_all_columns = ['t', 'x', 'y', 'z']\n",
    "fluidpump_fs_hz = 26866\n",
    "fluidpump_spectral_window = 2 ** 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluidpump_csv_import(dataset: ZipFile, filename: str) -> pd.DataFrame:\n",
    "    ts = pd.read_csv(\n",
    "        dataset.open(filename),\n",
    "        delimiter='\\t',\n",
    "        index_col=False,\n",
    "        header=0,\n",
    "        names=fluidpump_all_columns\n",
    "    ) \n",
    "    g = 9.80665\n",
    "    columns = fluidpump_columns\n",
    "    ts[columns] = ts[columns].apply(lambda x: g * (x / 1000))\n",
    "\n",
    "    T = 1 / fluidpump_fs_hz\n",
    "    ts = (\n",
    "        ts\n",
    "        .assign(t = lambda x: x.index * T)\n",
    "        .assign(key=filename)\n",
    "    )\n",
    "    return ts\n",
    "\n",
    "\n",
    "def fluidpump_features_by_domain(\n",
    "        features_calc: Callable,\n",
    "        dataset: ZipFile,\n",
    "        filename: str, \n",
    "        parts: int = None\n",
    "    ):\n",
    "\n",
    "    # print(f'Processing: {filename}')\n",
    "    fs = fluidpump_fs_hz\n",
    "    columns = fluidpump_columns\n",
    "    window = fluidpump_spectral_window\n",
    "\n",
    "    ts = fluidpump_csv_import(dataset, filename)\n",
    "    dataframe = split_dataframe(ts, parts)\n",
    "    dataframe = detrending_filter(dataframe, columns)\n",
    "    \n",
    "    header = filename.split(os.path.sep)\n",
    "    metadata = [\n",
    "        ('place', [header[-5]]),\n",
    "        ('date', [datetime.fromisoformat(header[-4]).date()]),\n",
    "        ('device', [header[-3]]),\n",
    "        ('position', [header[-2]]),\n",
    "        ('seq', [int(header[-1].split('.')[0])])\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for i, df in enumerate(dataframe):\n",
    "        fvector = metadata.copy()\n",
    "        for col in columns:\n",
    "            fvector.extend(features_calc(df, col, fs, window))\n",
    "        result.append(pd.DataFrame(dict(fvector))) \n",
    "\n",
    "    return pd.concat(result).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction procedure and save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_time_domain(dataset: ZipFile, filename: str, parts: int) -> pd.DataFrame:\n",
    "    if opt['name'] == 'mafaulda':\n",
    "        return mafaulda_features_by_domain(time_features_calc, dataset, filename, parts)\n",
    "    elif opt['name'] == 'fluidpump':\n",
    "        return fluidpump_features_by_domain(time_features_calc, dataset, filename, parts)\n",
    "\n",
    "\n",
    "def features_frequency_domain(dataset: ZipFile, filename: str, parts: int) -> pd.DataFrame:\n",
    "    if opt['name'] == 'mafaulda':\n",
    "        return mafaulda_features_by_domain(frequency_features_calc, dataset, filename, parts)\n",
    "    elif opt['name'] == 'fluidpump':\n",
    "        return fluidpump_features_by_domain(frequency_features_calc, dataset, filename, parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features\n",
    "features = load_files_split(\n",
    "    dataset=ZipFile(opt['dataset']),\n",
    "    func=features_time_domain,\n",
    "    parts=opt['parts']\n",
    ")\n",
    "features.to_csv(opt['temporal_features'], index=False)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral features\n",
    "features = load_files_split(\n",
    "    dataset=ZipFile(opt['dataset']),\n",
    "    func=features_frequency_domain,\n",
    "    parts=opt['parts']\n",
    ")\n",
    "features.to_csv(opt['spectral_features'], index=False)\n",
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
