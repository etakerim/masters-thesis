{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling from extracted features,  .assign(rpm = lambda x: 1500)  # TODO: PodÄ¾a typu zariadenia\n",
    "# Count classes\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List, Set, Tuple, Dict\n",
    "from river import feature_selection, stream, preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "from enum import Enum, auto\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from vibrodiagnostics import selection, models, ranking\n",
    "\n",
    "\n",
    "PATH_PREFIX = '../../datasets/'\n",
    "FEATURES_PATH =  os.path.join(PATH_PREFIX, 'features_data')\n",
    "\n",
    "TD_FD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_AND_FREQ_FEATURES_PATH)\n",
    "TD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_FEATURES_PATH)\n",
    "FD_FEATURES = os.path.join(FEATURES_PATH, selection.FREQ_FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = {\n",
    "    'A': {\n",
    "        'normal': 'normal',\n",
    "        'imbalance': 'imbalance',\n",
    "        'horizontal-misalignment': 'misalignment',\n",
    "        'vertical-misalignment': 'misalignment',\n",
    "        'underhang-outer_race': 'outer race fault',\n",
    "        'underhang-cage_fault': 'cage fault',\n",
    "        'underhang-ball_fault': 'ball fault'\n",
    "    },\n",
    "    'B': {\n",
    "        'normal': 'normal',\n",
    "        'imbalance': 'imbalance',\n",
    "        'horizontal-misalignment': 'misalignment',\n",
    "        'vertical-misalignment': 'misalignment',\n",
    "        'overhang-cage_fault': 'cage fault',\n",
    "        'overhang-ball_fault': 'ball fault',\n",
    "        'overhang-outer_race': 'outer race fault'\n",
    "    }\n",
    "}\n",
    "\n",
    "placements = {\n",
    "    'A': ['ax', 'ay', 'az'],\n",
    "    'B': ['bx', 'by', 'bz']\n",
    "}\n",
    "\n",
    "\n",
    "domains = {'temporal': TD_FEATURES, 'spectral': FD_FEATURES}\n",
    "target = ['fault', 'anomaly_80']\n",
    "placement = ['A', 'B']\n",
    "online = [False, True]\n",
    "GENERATE = False\n",
    "\n",
    "\n",
    "def get_features_list(domains):\n",
    "    features = []\n",
    "    for dname, dataset in domains.items():\n",
    "        names = pd.read_csv(dataset)\n",
    "        names = names.columns.str.extract(r'([a-z]{2})_([a-z\\_\\-]+)')[1].unique()\n",
    "        features.extend([f'{dname}_{col.strip(\"_\")}' for col in names if not pd.isnull(col)])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "temporal_columns = get_features_list({'temporal': TD_FEATURES})\n",
    "spectral_columns = get_features_list({'spectral': FD_FEATURES})\n",
    "all_columns = temporal_columns + spectral_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(dataset: str, domain: str, row: dict, all: bool = False, window_size=2**14):\n",
    "    features = pd.read_csv(dataset).fillna(0)\n",
    "\n",
    "    if not all:\n",
    "        # Labeling anomaly severity levels\n",
    "        target = re.search(r'([a-z]+)_?(\\d+)?', row['target'])\n",
    "        anomaly_severity = target.group(2) or '60'\n",
    "        anomaly_severity = float(anomaly_severity) / 100\n",
    "\n",
    "        # Choose measurement placement: A or B\n",
    "        place = row['placement']\n",
    "        axis = placements[place]\n",
    "        features = features[features['fault'].isin(tuple(faults[place]))]\n",
    "        features = models.fault_labeling(features, faults[place], anomaly_severity)\n",
    "\n",
    "        columns = features.columns.str.startswith(tuple(axis))\n",
    "        X = features[features.columns[columns]]\n",
    "\n",
    "        # Select predicted variable column\n",
    "        label = target.group(1)\n",
    "        Y = features[label].astype('category')\n",
    "    else:\n",
    "        axis = placements['A']\n",
    "        columns = features.columns.str.startswith(tuple(axis))\n",
    "        X = features[features.columns[columns]]\n",
    "        Y = features['fault'].astype('category')\n",
    "\n",
    "\n",
    "    # Filter columns in feature domain with window size 2**14\n",
    "    if domain == 'spectral':\n",
    "        X = X.loc[:,X.columns.str.endswith(f'_{window_size}')]\n",
    "        X.columns = X.columns.str.extract(r'(\\w+)_\\w+$')[0]\n",
    "\n",
    "    # Calculate feature magnitudes from 3D vector\n",
    "    feature_names = get_features_list({domain: dataset})\n",
    "    result = pd.DataFrame()\n",
    "    for name in feature_names:              \n",
    "        # Remove prefix: temporal, spectral\n",
    "        name = re.search(r'[a-z]+_([\\w\\_]+)', name).group(1)\n",
    "        vector_dims = [f'{dim}_{name}' for dim in axis]\n",
    "        result[name] = X[vector_dims].apply(np.linalg.norm, axis=1)\n",
    "    X = result\n",
    "\n",
    "    # Batch / Online hold-out (balance and event sequencing)\n",
    "    train_size = 0.8\n",
    "    if row['online']:\n",
    "        # Shuffle order within severity level and order event with increasing severity\n",
    "        groups = [\n",
    "            df.sample(frac=1, random_state=10)\n",
    "            for i, df in (\n",
    "                features.sort_values(by='severity_level').groupby('severity_level')\n",
    "            )\n",
    "        ]\n",
    "        rows = list(pd.concat(groups).index)\n",
    "        X = X.loc[rows].reset_index(drop=True)\n",
    "        Y = Y.loc[rows].reset_index(drop=True)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, train_size=train_size, random_state=10\n",
    "        )   \n",
    "        X_train, X_test, Y_train, Y_test = (\n",
    "            X_train.sort_index(), X_test.sort_index(),\n",
    "            Y_train.sort_index(), Y_test.sort_index()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy='not majority', random_state=10)\n",
    "        X, Y = oversample.fit_resample(X, Y.to_numpy())\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        Y = pd.Series(Y)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, train_size=train_size, stratify=Y, random_state=10\n",
    "        )\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def run_experiments(conditions: List[dict], exp_output: ranking.ExperimentOutput, pc=3) -> pd.DataFrame:\n",
    "    experiments = []\n",
    "\n",
    "    for row in tqdm(conditions):\n",
    "        experiment = row.copy()\n",
    "\n",
    "        for domain_label, dataset in domains.items():\n",
    "            X_train, X_test, Y_train, Y_test = load_source(dataset, domain_label, row)\n",
    "\n",
    "            # Count samples\n",
    "            if exp_output == ranking.ExperimentOutput.COUNTS:\n",
    "                experiment.update({'n_train': len(X_train), 'n_test': len(X_test), 'sum': len(X)})\n",
    "                break\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.PCA:\n",
    "                experiment = row.copy()\n",
    "                experiment.update({'domain': domain_label})\n",
    "                experiment.update(ranking.pca_explained_variances(X_train, pc))\n",
    "                experiments.append(experiment)\n",
    "                continue\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.SILHOUETTE:\n",
    "                synonyms = ranking.compute_correlations(X_train, corr_above=0.95)\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train)\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train)\n",
    "    \n",
    "                best_features = ranking.best_columns(ranks, synonyms, n=3)\n",
    "                scores = ranking.silhouette_scores(X_train, X_test, Y_train, Y_test, best_features, pc)\n",
    "                experiment = row.copy()\n",
    "                experiment.update({'domain': domain_label})\n",
    "                experiment.update(scores)\n",
    "                experiments.append(experiment)\n",
    "                continue\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.BEST_SET:\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train)\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train)\n",
    "                synonyms = ranking.compute_correlations(X_train, corr_above=0.95)\n",
    "                subset = ranking.best_subset(ranks, synonyms, n=3)\n",
    "                output = subset\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.BEST_CORR:\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train, 'corr')\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train, 'corr')\n",
    "                synonyms = ranking.compute_correlations(X_train, corr_above=0.95)\n",
    "                subset = ranking.best_subset(ranks, synonyms, n=3)\n",
    "                output = subset\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.BEST_F_STAT:\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train, 'f_stat')\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train,'f_stat')\n",
    "                synonyms = ranking.compute_correlations(X_train, corr_above=0.95)\n",
    "                subset = ranking.best_subset(ranks, synonyms, n=3)\n",
    "                output = subset\n",
    "\n",
    "            elif exp_output == ranking.ExperimentOutput.BEST_MI:\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train, 'mi')\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train, 'mi')\n",
    "                synonyms = ranking.compute_correlations(X_train, corr_above=0.95)\n",
    "                subset = ranking.best_subset(ranks, synonyms, n=3)\n",
    "                output = subset\n",
    "    \n",
    "            elif exp_output == ranking.ExperimentOutput.RANKS:\n",
    "                if row['online']:\n",
    "                    ranks = ranking.online_feature_ranking(X_train, Y_train)\n",
    "                else:\n",
    "                    ranks = ranking.batch_feature_ranking(X_train, Y_train)\n",
    "                output = ranks\n",
    "\n",
    "            output.reset_index(inplace=True)\n",
    "            output['feature'] = output['feature'].apply(lambda s: f'{domain_label}_{s}')\n",
    "            output = dict(zip(list(output['feature']), list(output['rank'])))\n",
    "            experiment.update(output)\n",
    "\n",
    "        if exp_output not in (ranking.ExperimentOutput.PCA, ranking.ExperimentOutput.SILHOUETTE):\n",
    "            experiments.append(experiment)\n",
    "\n",
    "    return pd.DataFrame.from_records(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['target', 'placement', 'online']\n",
    "initial_conditions = [\n",
    "    dict(zip(column_names, row)) \n",
    "    for row in itertools.product(target, placement, online)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority voting: feature in subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 member sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    membership = run_experiments(initial_conditions, ranking.ExperimentOutput.BEST_SET)\n",
    "    membership.to_csv('best_set/rank_product.csv', index=False)\n",
    "    membership = run_experiments(initial_conditions, ranking.ExperimentOutput.BEST_CORR)\n",
    "    membership.to_csv('best_set/corr.csv', index=False)\n",
    "    membership = run_experiments(initial_conditions, ranking.ExperimentOutput.BEST_F_STAT)\n",
    "    membership.to_csv('best_set/fstat.csv', index=False)\n",
    "    membership = run_experiments(initial_conditions, ranking.ExperimentOutput.BEST_MI)\n",
    "    membership.to_csv('best_set/mi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally best features (batch and online)\n",
    "def globally_best_batch_features(filename):\n",
    "    best_set_membership = pd.read_csv(filename)\n",
    "\n",
    "    group = best_set_membership[best_set_membership['target'] == 'fault']  # anomaly_90\n",
    "    for i, col in enumerate([temporal_columns, spectral_columns]):\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        graph = group[col][group == True].count(axis=0).sort_values(ascending=False)\n",
    "        plt.grid()\n",
    "        ax.bar([re.search('[a-z]+_(\\w+)', s).group(1) for s in graph.index], graph)\n",
    "        ax.set_xlabel('Feature')\n",
    "        ax.set_ylabel('Count of best subset memberships')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globally_best_batch_features('best_set/rank_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globally_best_batch_features('best_set/corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globally_best_batch_features('best_set/fstat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globally_best_batch_features('best_set/mi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set_membership = pd.read_csv('best_set/rank_product.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_membership.groupby(by=['online']):\n",
    "    t_situation = group[temporal_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "    f_situation = group[spectral_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "    agg[key] = pd.concat([t_situation, f_situation]).index\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set_membership = pd.read_csv('best_set/rank_product.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_membership.groupby(by=['online', 'target']):\n",
    "    t_situation = group[temporal_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "    f_situation = group[spectral_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "    agg[key] = pd.concat([t_situation, f_situation]).index\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank product: feature ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    best_set_ranks = run_experiments(initial_conditions, ranking.ExperimentOutput.RANKS)    # 6 minutes\n",
    "    best_set_ranks.to_csv('best_set/ranks.csv', index=False)\n",
    "    best_set_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set_ranks = pd.read_csv('best_set/ranks.csv')\n",
    "# Globally best features (lower rank is better)\n",
    "\n",
    "group = best_set_ranks[best_set_ranks['online'] == False]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "for i, col in enumerate([temporal_columns, spectral_columns]):\n",
    "    graph = group[col].apply(gmean, axis=0).sort_values(ascending=True)\n",
    "    print(graph)\n",
    "    ax[i].grid()\n",
    "    ax[i].bar([re.search('[a-z]+_(\\w+)', s).group(1) for s in graph.index], graph)\n",
    "plt.show()\n",
    "\n",
    "# Online\n",
    "group = best_set_ranks[best_set_ranks['online'] == True]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "for i, col in enumerate([temporal_columns, spectral_columns]):\n",
    "    graph = group[col].apply(gmean, axis=0).sort_values(ascending=True)\n",
    "    print(graph)\n",
    "    ax[i].grid()\n",
    "    ax[i].bar([re.search('[a-z]+_(\\w+)', s).group(1) for s in graph.index], graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary (absolute counts) - RPM limited/unlimted and machinery element\n",
    "best_set_ranks = pd.read_csv('best_set/ranks.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_ranks.groupby(by=['online']):\n",
    "    agg[key] = group[all_columns].apply(gmean, axis=0)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set_ranks = pd.read_csv('best_set/ranks.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_ranks.groupby(by=['online']):\n",
    "    t_situation = group[temporal_columns].apply(gmean, axis=0).sort_values(ascending=True).head(3)\n",
    "    f_situation = group[spectral_columns].apply(gmean, axis=0).sort_values(ascending=True).head(3)\n",
    "    agg[key] = pd.concat([t_situation, f_situation]).index\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary (absolute counts) - RPM limited/unlimted and machinery element\n",
    "best_set_ranks = pd.read_csv('best_set/ranks.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_ranks.groupby(by=['online', 'target']):\n",
    "    agg[key] = group[all_columns].apply(gmean, axis=0)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_set_ranks = pd.read_csv('best_set/ranks.csv')\n",
    "agg = pd.DataFrame()\n",
    "for key, group in best_set_ranks.groupby(by=['online', 'target']):\n",
    "    t_situation = group[temporal_columns].apply(gmean, axis=0).sort_values(ascending=False).head(3)\n",
    "    f_situation = group[spectral_columns].apply(gmean, axis=0).sort_values(ascending=False).head(3)\n",
    "    agg[key] = pd.concat([t_situation, f_situation]).index\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best features by experiment\n",
    "- Majority voting\n",
    "- Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_names(feature_set):\n",
    "    return [re.search('[a-z]+_(\\w+)', s).group(1) for s in feature_set.index]\n",
    "\n",
    "def best_featue_set_methods(filename):\n",
    "    best_set_membership = pd.read_csv(filename)\n",
    "    feature_sets = []\n",
    "    indexer = ['placement', 'online', 'target']\n",
    "    for key, group in best_set_membership.groupby(by=indexer):\n",
    "        t_situation = group[temporal_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "        f_situation = group[spectral_columns][group == True].count(axis=0).sort_values(ascending=False).head(3)\n",
    "\n",
    "        # Extract feature names\n",
    "        temporal = list(sorted(extract_feature_names(t_situation)))\n",
    "        spectral = list(sorted(extract_feature_names(f_situation)))\n",
    "\n",
    "        fset = {'placement': key[0], 'online': key[1], 'target': key[2], 'temporal': temporal , 'spectral': spectral}\n",
    "        feature_sets.append(fset)\n",
    "\n",
    "    return pd.DataFrame.from_records(feature_sets).set_index(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_featue_set_methods('best_set/rank_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_featue_set_methods('best_set/corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_featue_set_methods('best_set/fstat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_featue_set_methods('best_set/mi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_names(feature_set):\n",
    "    return [re.search('[a-z]+_(\\w+)', s).group(1) for s in feature_set.index]\n",
    "\n",
    "best_set_membership = pd.read_csv('best_set/ranks.csv')\n",
    "feature_sets = []\n",
    "indexer = ['placement', 'online', 'target']\n",
    "for key, group in best_set_membership.groupby(by=indexer):\n",
    "    t_situation = group[temporal_columns].apply(gmean, axis=0).sort_values(ascending=False).head(3)\n",
    "    f_situation = group[spectral_columns].apply(gmean, axis=0).sort_values(ascending=False).head(3)\n",
    "\n",
    "    # Extract feature names\n",
    "    temporal = list(sorted(extract_feature_names(t_situation)))\n",
    "    spectral = list(sorted(extract_feature_names(f_situation)))\n",
    "\n",
    "    fset = {'placement': key[0], 'online': key[1], 'target': key[2], 'temporal': temporal , 'spectral': spectral}\n",
    "    feature_sets.append(fset)\n",
    "\n",
    "pd.DataFrame.from_records(feature_sets).set_index(indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA explained variance (batch only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['target', 'placement', 'online']\n",
    "batch_initial_conditions = [\n",
    "    dict(zip(column_names, row)) \n",
    "    for row in itertools.product(target, placement, [False])\n",
    "]\n",
    "pca_vars = run_experiments(batch_initial_conditions, ranking.ExperimentOutput.PCA)\n",
    "pca_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variances(pca_vars_in):\n",
    "    selected_columns = ['target', 'PC1', 'PC2', 'PC3']\n",
    "    groupby_columns = ['target']\n",
    "\n",
    "    pca_A_temporal = pca_vars_in[\n",
    "        (pca_vars_in['placement'] == 'A') & (pca_vars_in['domain'] == 'temporal')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "    pca_B_temporal = pca_vars_in[\n",
    "        (pca_vars_in['placement'] == 'B') & (pca_vars_in['domain'] == 'temporal')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "\n",
    "    pca_A_spectral = pca_vars_in[\n",
    "        (pca_vars_in['placement'] == 'A') & (pca_vars_in['domain'] == 'spectral')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "    pca_B_spectral = pca_vars_in[\n",
    "        (pca_vars_in['placement'] == 'B') & (pca_vars_in['domain'] == 'spectral')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    pca_A_temporal.plot.bar(stacked=True, grid=True, ax=ax[0][0], title='Temporal features, Placement: A', xlabel='', ylabel='Explained variance')\n",
    "    pca_B_temporal.plot.bar(stacked=True, grid=True, ax=ax[0][1], title='Temporal features, Placement: B', xlabel='', ylabel='Explained variance')\n",
    "    pca_A_spectral.plot.bar(stacked=True, grid=True, ax=ax[1][0], title='Spectral features, Placement: A', xlabel='', ylabel='Explained variance')\n",
    "    pca_B_spectral.plot.bar(stacked=True, grid=True, ax=ax[1][1], title='Spectral features, Placement: B', xlabel='', ylabel='Explained variance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# No RPM limit\n",
    "plot_explained_variances(pca_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = run_experiments(batch_initial_conditions, ExperimentOutput.SILHOUETTE)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette_scores(scores, cols):\n",
    "    selected_columns = ['target'] + cols\n",
    "    groupby_columns = ['target']\n",
    "\n",
    "    scores_A_temporal = scores[\n",
    "        (scores['placement'] == 'A') & (scores['domain'] == 'temporal')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "    scores_B_temporal = scores[\n",
    "        (scores['placement'] == 'B') & (scores['domain'] == 'temporal')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "\n",
    "    scores_A_spectral = scores[\n",
    "        (scores['placement'] == 'A') & (scores['domain'] == 'spectral')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "    scores_B_spectral = scores[\n",
    "        (scores['placement'] == 'B') & (scores['domain'] == 'spectral')\n",
    "    ][selected_columns].set_index(groupby_columns)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    scores_A_temporal.plot.bar(grid=True, ax=ax[0][0], title='Temporal features, Placement: A', xlabel='', ylabel='Silhouette score')\n",
    "    scores_B_temporal.plot.bar(grid=True, ax=ax[0][1], title='Temporal features, Placement: B', xlabel='', ylabel='Silhouette variance')\n",
    "    scores_A_spectral.plot.bar(grid=True, ax=ax[1][0], title='Spectral features, Placement: A', xlabel='', ylabel='Silhouette variance')\n",
    "    scores_B_spectral.plot.bar(grid=True, ax=ax[1][1], title='Spectral features, Placement: B', xlabel='', ylabel='Silhouette variance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_silhouette_scores(scores, ['train', 'test'])\n",
    "#plt.suptitle('Best features, all RPM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_scores(scores, ['train_pca', 'test_pca'])\n",
    "plt.suptitle('PCA, all RPM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = run_experiments(initial_conditions, ExperimentOutput.COUNTS)  \n",
    "counters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature distribution in different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_features(conditions: List[dict]):\n",
    "    for row in tqdm(conditions):\n",
    "        experiment = row.copy()\n",
    "        print(row)\n",
    "\n",
    "        for domain_label, dataset in domains.items():\n",
    "            X_train, X_test, Y_train, Y_test = load_source(dataset, domain_label, row)\n",
    "\n",
    "            # MinMax scaled result\n",
    "            # scaler = MinMaxScaler()\n",
    "            # X_train_scaled = pd.DataFrame()\n",
    "            # X_train_scaled[X_train.columns] = scaler.fit_transform(X_train)\n",
    "            X_train_scaled = X_train\n",
    "\n",
    "            # Diagonal of covariance matrix to see explained variance cov(A, A) = var(A)\n",
    "            # Variance threshold\n",
    "            train_cov = X_train_scaled.cov()\n",
    "            diagonal_cov = pd.Series(np.diag(train_cov), index=[train_cov.index, train_cov.columns])\n",
    "            diagonal_cov = diagonal_cov / diagonal_cov.sum()\n",
    "            diagonal_cov = diagonal_cov.sort_values(ascending=False)\n",
    "            print(row)\n",
    "            print(diagonal_cov)\n",
    "\n",
    "            X_train_scaled['target'] = Y_train\n",
    "            # Show boxplots split by predicted variable\n",
    "            X_train_scaled.boxplot(figsize=(15, 5))\n",
    "            plt.show()\n",
    "            X_train_scaled.boxplot(figsize=(20, 5), layout=(2, 6), by='target', sharey=False)\n",
    "            plt.show()\n",
    "\n",
    "column_names = ['target', 'placement', 'online']\n",
    "batch_initial_conditions = [\n",
    "    dict(zip(column_names, row)) \n",
    "    for row in itertools.product(target, placement, [False])\n",
    "]\n",
    "boxplot_features(batch_initial_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whole_dataset(dataset: str, domain: str, window_size=2**14):\n",
    "    features = pd.read_csv(dataset).fillna(0)\n",
    "\n",
    "    axis = placements['A']\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]]\n",
    "    Y = features['rpm'].astype('category')\n",
    "\n",
    "    # Filter columns in feature domain with window size 2**14\n",
    "    if domain == 'spectral':\n",
    "        X = X.loc[:,X.columns.str.endswith(f'_{window_size}')]\n",
    "        X.columns = X.columns.str.extract(r'(\\w+)_\\w+$')[0]\n",
    "\n",
    "    # Calculate feature magnitudes from 3D vector\n",
    "    feature_names = get_features_list({domain: dataset})\n",
    "    result = pd.DataFrame()\n",
    "    for name in feature_names:              \n",
    "        # Remove prefix: temporal, spectral\n",
    "        name = re.search(r'[a-z]+_([\\w\\_]+)', name).group(1)\n",
    "        vector_dims = [f'{dim}_{name}' for dim in axis]\n",
    "        result[name] = X[vector_dims].apply(np.linalg.norm, axis=1)\n",
    "    X = result\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxprops = dict(linewidth=1, color='k')\n",
    "medianprops = dict(linewidth=2, color='k')\n",
    "X, Y = load_whole_dataset(TD_FEATURES, 'temporal')\n",
    "X.plot(\n",
    "    kind='box', \n",
    "    subplots=True, \n",
    "    sharey=False, \n",
    "    figsize=(20, 5),\n",
    "    grid=True,\n",
    "    boxprops=boxprops,\n",
    "    medianprops=medianprops,\n",
    "    whiskerprops=boxprops,\n",
    "    capprops=boxprops\n",
    "    \n",
    ")\n",
    "\n",
    "corrs = {}\n",
    "for col in X.columns:\n",
    "    corrs[col] = np.corrcoef(X[col], Y)[0, 1]\n",
    "\n",
    "plt.subplots_adjust(wspace=0.6) \n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "#pd.DataFrame.from_records([corrs]).T.describe()\n",
    "\n",
    "x_scaled = pd.DataFrame()\n",
    "x_scaled[X.columns] = MinMaxScaler().fit_transform(X)\n",
    "vars = {}\n",
    "\n",
    "X_td = X.copy()\n",
    "pca_td = PCA(n_components=10)\n",
    "X_pca = pca_td.fit_transform(x_scaled)\n",
    "print(pca_td.explained_variance_ratio_)\n",
    "print(np.cumsum(pca_td.explained_variance_ratio_))\n",
    "\n",
    "for col in x_scaled.columns:\n",
    "    vars[col] = np.var(x_scaled[col])\n",
    "vars = pd.DataFrame.from_records([vars]).T\n",
    "\n",
    "\n",
    "(100 * (vars / vars.sum())).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_SIZE = 2**14\n",
    "X, Y = load_whole_dataset(FD_FEATURES, 'spectral', WIN_SIZE)\n",
    "X.plot(\n",
    "    kind='box', \n",
    "    subplots=True, \n",
    "    sharey=False, \n",
    "    figsize=(20, 5),\n",
    "    grid=True,\n",
    "    boxprops=boxprops,\n",
    "    medianprops=medianprops,\n",
    "    whiskerprops=boxprops,\n",
    "    capprops=boxprops\n",
    "    \n",
    ")\n",
    "corrs = {}\n",
    "for col in X.columns:\n",
    "    corrs[col] = np.corrcoef(X[col], Y)[0, 1]\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5) \n",
    "plt.show()\n",
    "#pd.DataFrame.from_records([corrs]).T.sort_values(by=0) #.describe()\n",
    "#covs = pd.DataFrame([cov]).T\n",
    "\n",
    "\n",
    "\n",
    "x_scaled = pd.DataFrame()\n",
    "x_scaled[X.columns] = MinMaxScaler().fit_transform(X)\n",
    "vars = {}\n",
    "\n",
    "X_fd = X.copy()\n",
    "pca_fd = PCA(n_components=10)\n",
    "X_pca = pca_fd.fit_transform(x_scaled)\n",
    "print(pca_fd.explained_variance_ratio_)\n",
    "print(np.cumsum(pca_fd.explained_variance_ratio_))\n",
    "\n",
    "for col in x_scaled.columns:\n",
    "    vars[col] = np.var(x_scaled[col])\n",
    "vars = pd.DataFrame.from_records([vars]).T\n",
    "\n",
    "# Explained variances\n",
    "(100 * (vars / vars.sum())).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained varinace by PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(np.arange(1, 11), 100 * np.cumsum(pca_td.explained_variance_ratio_), marker='s', label='Temporal features')\n",
    "ax.plot(np.arange(1, 11), 100 * np.cumsum(pca_fd.explained_variance_ratio_), marker='s', label='Spectral features')\n",
    "ax.set_xlabel('Number of principal components')\n",
    "ax.set_ylabel('Percentage of explained variance')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadings plot\n",
    "- https://www.jcchouinard.com/python-pca-biplots-machine-learning/\n",
    "- https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistical-modeling/multivariate/how-to/principal-components/interpret-the-results/key-results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca_td.components_\n",
    "n_features = pca_td.n_features_in_\n",
    "feature_names = X_td.columns\n",
    "pc_list = [f'PC{i}' for i in list(range(1, n_features + 1))]\n",
    "\n",
    "# Match PC names to loadings\n",
    "pc_loadings = dict(zip(pc_list, loadings))\n",
    "\n",
    "# Matrix of corr coefs between feature names and PCs\n",
    "loadings_df = pd.DataFrame.from_dict(pc_loadings)\n",
    "loadings_df['feature_names'] = feature_names\n",
    "loadings_df = loadings_df.set_index('feature_names')\n",
    "loadings_df[['PC1', 'PC2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "\n",
    "def loading_plot(loadings, feature_names, bottom, top):\n",
    "    xs = loadings[0]\n",
    "    ys = loadings[1]\n",
    "\n",
    "    texts = []\n",
    "    # Plot the loadings on a scatterplot\n",
    "    for i, varnames in enumerate(feature_names):\n",
    "        plt.arrow(\n",
    "            0, 0, # coordinates of arrow base\n",
    "            xs[i], # length of the arrow along x\n",
    "            ys[i], # length of the arrow along y\n",
    "            color='r', \n",
    "            head_width=0.01\n",
    "        )\n",
    "        texts.append(plt.text(xs[i], ys[i], varnames))\n",
    "\n",
    "    # Define the axes\n",
    "    adjust_text(texts, only_move={'points':'y', 'texts':'y'})\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.xlim(bottom, top)\n",
    "    plt.ylim(bottom, top)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "loading_plot(pca_td.components_, X_td.columns, -0.5, 1)\n",
    "plt.show()\n",
    "loading_plot(pca_fd.components_, X_fd.columns, -0.5, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
