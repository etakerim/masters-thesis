{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN\n",
    "- https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from feature.selection import load_td_feat, load_fd_feat\n",
    "from feature.models import (\n",
    "    fault_labeling, pipeline_v1, pipeline_v1_core, filter_out_metadata_columns\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "FEATURES_PATH =  '../../datasets/features_data/'\n",
    "FAULT_CLASSES = {\n",
    "    'normal': 'N',\n",
    "    'imbalance': 'I',\n",
    "    'horizontal-misalignment': 'HM',\n",
    "    'vertical-misalignment': 'VM'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fd_feat(['ax', 'ay', 'az'], path=FEATURES_PATH)\n",
    "dataset = fault_labeling(dataset, FAULT_CLASSES)\n",
    "X_train, X_test, y_train, y_test = pipeline_v1(dataset, train=0.6, nfeat=3, func_select=mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distances among points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_neighbors = 6\n",
    "neighbors = NearestNeighbors(n_neighbors=cnt_neighbors)\n",
    "neighbors.fit(X_train)\n",
    "distances, indices = neighbors.kneighbors(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plot distances among points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "distance_desc = sorted(distances[:, 1], reverse=True)\n",
    "ax.plot(list(range(1, len(distance_desc) + 1)), distance_desc)\n",
    "ax.set_xlabel('Number of points')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneedle = KneeLocator(range(1, len(distance_desc) + 1), distance_desc,\n",
    "                      S=1.0, curve='convex', direction='decreasing')\n",
    "kneedle.plot_knee_normalized()\n",
    "print(kneedle.elbow, kneedle.knee_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cuts_3d_cluster(X_train, cluster, title):\n",
    "    df = X_train.copy()\n",
    "    df['cluster'] = cluster\n",
    "    df['cluster'] = df['cluster'].astype('category')\n",
    "\n",
    "    categories = df['cluster'].cat.categories\n",
    "    colors = sb.color_palette('hls', len(categories))\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i, axes in enumerate(((0, 1), (0, 2), (1, 2))):\n",
    "        a, b = axes\n",
    "         \n",
    "        for label, color in zip(categories, colors):\n",
    "            rows = list(df[df['cluster'] == label].index)\n",
    "            x = df.loc[rows, df.columns[a]]\n",
    "            y = df.loc[rows, df.columns[b]]\n",
    "            ax[i].scatter(x, y, s=1, color=color, label=label)\n",
    "\n",
    "        ax[i].set_xlabel(df.columns[a])\n",
    "        ax[i].set_ylabel(df.columns[b])\n",
    "        ax[i].grid()\n",
    "        ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Range of values is MinMaxScaled in range (0, 1) - eps must be smaller than 1\n",
    "- Noisy samples are given the label -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_td_feat(['ax', 'ay', 'az'], path=FEATURES_PATH)\n",
    "dataset = fault_labeling(dataset, FAULT_CLASSES)\n",
    "X_train, X_test, y_train, y_test = pipeline_v1(dataset, train=0.6, nfeat=3, func_select=mutual_info_classif)\n",
    "print('Features:', list(X_train.columns))\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train, 'Ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.1, min_samples=5, metric='l2')\n",
    "clustering.fit(X_train)\n",
    "y_train_labels = clustering.labels_\n",
    "y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train_labels, 'Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clustering(X_train, y_train_labels, X_test, y_predict):\n",
    "    # The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "    # The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters\n",
    "    print('Silhouette score:')\n",
    "    print('Train:', silhouette_score(X_train, y_train_labels, metric='euclidean'))\n",
    "    print('Test:', silhouette_score(X_test, y_predict, metric='euclidean'))\n",
    "\n",
    "    # Daviesâ€“Bouldin index: The minimum score is zero, with lower values indicating better clustering.\n",
    "    print('Davies-Bouldin index')\n",
    "    print('Train:', davies_bouldin_score(X_train, y_train_labels))\n",
    "    print('Test:', davies_bouldin_score(X_test, y_predict))\n",
    "\n",
    "    occurences = pd.DataFrame(\n",
    "        data=contingency_matrix(y_train, y_train_labels),\n",
    "        index=np.unique(y_train),\n",
    "        columns=np.unique(y_train_labels)\n",
    "    )\n",
    "    ax = sb.heatmap(occurences, cbar=True, cmap='BuGn', annot=True, fmt='d')\n",
    "\n",
    "\n",
    "evaluate_clustering(X_train, y_train_labels, X_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fd_feat(['ax', 'ay', 'az'], path=FEATURES_PATH)\n",
    "dataset = fault_labeling(dataset, FAULT_CLASSES)\n",
    "X_train, X_test, y_train, y_test = pipeline_v1(dataset, train=0.6, nfeat=3, func_select=mutual_info_classif)\n",
    "print('Features:', list(X_train.columns))\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train, 'Ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.1, min_samples=5, metric='l2')\n",
    "clustering.fit(X_train)\n",
    "y_train_labels = clustering.labels_\n",
    "y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "cross_cuts_3d_cluster(X_train, y_train_labels, 'Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_clustering(X_train, y_train_labels, X_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters for DBSCAN in supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_clustering_score(X, y, num_of_features, eps, min_samples):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    clusters = []\n",
    "\n",
    "    for train_index, test_index in crossvalid.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "        X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = pipeline_v1_core(\n",
    "            FSEL_METHOD, int(num_of_features), \n",
    "            X_train, y_train, X_test, y_test\n",
    "        )\n",
    "\n",
    "        clustering = DBSCAN(eps=eps, min_samples=int(min_samples), metric='l2')\n",
    "        clustering.fit(X_train)\n",
    "        y_train_labels = clustering.labels_\n",
    "        y_predict = clustering.fit_predict(X_test)\n",
    "\n",
    "        num_of_clusters = len(np.unique(y_train_labels))\n",
    "        clusters.append(num_of_clusters)\n",
    "        if num_of_clusters > 1:\n",
    "            train_scores.append(silhouette_score(X_train, y_train_labels, metric='euclidean'))\n",
    "            #test_scores.append(silhouette_score(X_test, y_predict, metric='euclidean'))\n",
    "\n",
    "    clusters = stats.mode(clusters).mode\n",
    "    train_scores = np.array(train_scores)\n",
    "    test_scores = np.array(test_scores)\n",
    "\n",
    "    clusters = np.mean(clusters)\n",
    "    train_score_mean = np.mean(train_scores)\n",
    "    train_score_std = np.std(train_scores)\n",
    "    #test_score_mean = np.mean(test_scores)\n",
    "    #test_score_std = np.std(test_scores)\n",
    "\n",
    "    return clusters, train_score_mean, train_score_std #test_score_mean, test_score_std\n",
    "\n",
    "\n",
    "TRAINING_SET_RATIO = 0.6\n",
    "FSEL_METHOD = mutual_info_classif   # f_classif, mutual_info_classif\n",
    "\n",
    "features = fault_labeling(load_fd_feat(['az'], path=FEATURES_PATH), FAULT_CLASSES)\n",
    "crossvalid = StratifiedKFold(n_splits=5)\n",
    "X = filter_out_metadata_columns(features)\n",
    "y = features['fault']\n",
    "\n",
    "\n",
    "num_of_features = np.arange(1, len(X.columns) + 1)\n",
    "eps = np.linspace(0.05, 0.8, 8)\n",
    "min_samples = np.linspace(3, 8, 2)\n",
    "\n",
    "grid = np.array(np.meshgrid(num_of_features, eps, min_samples)).T.reshape(-1, 3)\n",
    "\n",
    "rows = []\n",
    "for f, e, s in tqdm(grid):\n",
    "    row = [f, e, s]\n",
    "    row.extend(cross_validate_clustering_score(X, y, f, e, s))\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "results = pd.DataFrame(rows, columns=[\n",
    "    'num_of_features', 'eps', 'min_samples', 'clusters', \n",
    "    'train_score_mean', 'train_score_std'\n",
    "]).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 best scored parameters with silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['clusters'] > 1].sort_values(by='train_score_mean', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
