{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch features\n",
    "\n",
    "Exports (RPM limit = False / True):\n",
    "```\n",
    "    - SHAFT, A, fault\n",
    "    - SHAFT, B, fault\n",
    "    - BEARINGS, A, fault\n",
    "    - BEARINGS, B, fault\n",
    "\n",
    "    - SHAFT, A, anomaly, 0.6\n",
    "    - SHAFT, B, anomaly, 0.6\n",
    "    - BEARINGS, A, anomaly, 0.6\n",
    "    - BEARINGS, B, anomaly, 0.6\n",
    "\n",
    "    - SHAFT, A, anomaly, 0.9\n",
    "    - SHAFT, B, anomaly, 0.9\n",
    "    - BEARINGS, A, anomaly, 0.9\n",
    "    - BEARINGS, B, anomaly, 0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEMENTS = [['ax', 'ay', 'az'], ['bx', 'by', 'bz']]\n",
    "TARGETS = ['fault', 'anomaly']\n",
    "ANOMALY_SEVERITIES = [0.6, 0.9]\n",
    "SHAFT_FAULTS = {'normal': 'N', 'imbalance': 'I', 'horizontal-misalignment': 'HM', 'vertical-misalignment': 'VM'}\n",
    "BEARING_FAULTS = {'overhang-cage_fault': 'O-Cage', 'underhang-cage_fault': 'U-Cage',\n",
    "                  'underhang-ball_fault': 'U-Ball', 'overhang-ball_fault': 'O-Ball',\n",
    "                  'underhang-outer_race': 'U-Race', 'overhang-ball_fault': 'O-Race'}\n",
    "FAULT_TYPES = [SHAFT_FAULTS, BEARING_FAULTS]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "ONLINE = False\n",
    "\n",
    "\n",
    "PLACE = PLACEMENTS[0]\n",
    "VAR_TARGET = TARGETS[1]\n",
    "ANOMALY_SEVERITY = ANOMALY_SEVERITIES[0]       # If it is anomaly\n",
    "\n",
    "FAULT_CLASSES = FAULT_TYPES[0]\n",
    "RPM_LIMIT = True                         # False, True\n",
    "BALANCE = True                           # False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from vibrodiagnostics import selection, models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean, spearmanr, kendalltau\n",
    "\n",
    "from typing import Callable\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "PATH_PREFIX = '../../datasets/'\n",
    "FEATURES_PATH =  os.path.join(PATH_PREFIX, 'features_data')\n",
    "\n",
    "TD_FD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_AND_FREQ_FEATURES_PATH)\n",
    "TD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_FEATURES_PATH)\n",
    "FD_FEATURES = os.path.join(FEATURES_PATH, selection.FREQ_FEATURES_PATH)\n",
    "\n",
    "METRICS_ONLINE = (selection.Correlation, selection.FisherScore, selection.MutualInformation)\n",
    "METRICS_OFFLINE = (selection.corr_classif, f_classif, mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(x: pd.DataFrame, y: pd.DataFrame, metric: Callable) -> pd.DataFrame:\n",
    "    scores = metric(x, y)\n",
    "    if isinstance(scores, tuple):\n",
    "        scores = scores[0]\n",
    "    leaderboard = (\n",
    "        pd.DataFrame(zip(x.columns, scores), columns=['feature', 'score'])\n",
    "        .set_index('feature')\n",
    "        .sort_values(by='score', ascending=False)\n",
    "    )\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "def features_subset_offline(filename, classes, axis, label, train_size, anomaly_severity, balance, rpm_limit):\n",
    "    features = pd.read_csv(filename).fillna(0)\n",
    "    features = features[features['fault'].isin(classes)]\n",
    "    if rpm_limit:\n",
    "        RPM = 2900\n",
    "        RPM_RANGE = 500\n",
    "        features = features[features['rpm'].between(RPM - RPM_RANGE, RPM + RPM_RANGE, inclusive='both')]\n",
    "    \n",
    "    features = models.fault_labeling(features, classes, anomaly_severity)\n",
    "\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]]\n",
    "    y = features[label].astype('category')\n",
    "    print(y)\n",
    "\n",
    "    # Balance dataset & Normalize dataset (later)\n",
    "    if balance:\n",
    "        oversample = RandomOverSampler(sampling_strategy='not majority', random_state=10)\n",
    "        X, y = oversample.fit_resample(X, y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, stratify=y, random_state=10\n",
    "    )\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def axis_to_magnitudes(feature_set: str, axis: list, window_size: int) -> tuple:\n",
    "    x, y, _, _ = features_subset_offline(\n",
    "        feature_set,\n",
    "        FAULT_CLASSES,\n",
    "        axis,\n",
    "        VAR_TARGET,\n",
    "        TRAIN_SIZE,\n",
    "        ANOMALY_SEVERITY,\n",
    "        BALANCE,\n",
    "        RPM_LIMIT\n",
    "    )\n",
    "    if isinstance(window_size, int):\n",
    "        x = x.loc[:,x.columns.str.endswith(f'_{window_size}')]\n",
    "        x.columns = x.columns.str.extract(r'([\\w\\_]+)_(\\w+)$')[0]\n",
    "\n",
    "    feature_names = x.columns.str.extract(r'([a-z]{2})_([\\w\\_\\-]+)')[1].unique()\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "    for name in feature_names:\n",
    "        vector_dims = [f'{dim}_{name}' for dim in axis]\n",
    "        result[name] = x[vector_dims].apply(np.linalg.norm, axis=1)\n",
    "\n",
    "    return result, y\n",
    "\n",
    "\n",
    "def feature_ranking(feature_set: str, window_size=None):\n",
    "    axis = PLACE\n",
    "    metric_ranks = pd.DataFrame()\n",
    "\n",
    "    for metric_name, metric in zip(('corr', 'f_stat', 'mi'), METRICS_OFFLINE):\n",
    "        x, y = axis_to_magnitudes(feature_set, axis, window_size)\n",
    "        scores = calculate_scores(x, y, metric)\n",
    "        metric_ranks[metric_name] = scores\n",
    "\n",
    "    return metric_ranks\n",
    "\n",
    "\n",
    "def corr_among_features(feature_set, axis, window_size=None):\n",
    "    x, y = axis_to_magnitudes(feature_set, axis, window_size)\n",
    "\n",
    "    correlations = [\n",
    "        {'feature_1': k[0], 'feature_2': k[1], 'corr': v}\n",
    "        for k, v in x.corr().abs().stack().to_dict().items()\n",
    "        if k[0] != k[1]\n",
    "    ]\n",
    "    correlations = pd.DataFrame.from_records(correlations).sort_values(by='corr', ascending=False)\n",
    "    correlations[correlations['corr'] > 0.7]\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def plot_scores(metric_ranks, n=None):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    for i, col in enumerate(('corr', 'f_stat', 'mi')):\n",
    "        scores = metric_ranks[col].sort_values(ascending=False)\n",
    "        if n is not None:\n",
    "            scores = scores.iloc[:n]\n",
    "        ax[i].bar(scores.index, scores)\n",
    "\n",
    "    for i, col_name in enumerate(('Correlation', 'F statistic', 'Mutual information')):\n",
    "        ax[i].set_xticks(ax[i].get_xticks())\n",
    "        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        ax[i].grid()\n",
    "        ax[i].set_xlabel('Feature')\n",
    "        ax[i].set_ylabel(col_name)\n",
    "\n",
    "\n",
    "def ensemble_feature_ranking(scores: pd.DataFrame):\n",
    "    ranks = scores.rank(axis='rows', method='first', ascending=False)\n",
    "    return ranks.apply(gmean, axis=1).sort_values().to_frame(name='rank') # Rank product\n",
    "\n",
    "\n",
    "def rank_correlation(scores: pd.DataFrame):\n",
    "    # spearman's rho vs kendall's tau\n",
    "    ranks = scores.rank(axis='rows', method='first', ascending=False)\n",
    "    correlations = []\n",
    "    for a, b in itertools.combinations(ranks.columns, r=2):\n",
    "        coef, pval = spearmanr(ranks[a], ranks[b])\n",
    "        # coef, pval = kendalltau(ranks[a], ranks[b])\n",
    "        correlations.append({'A': a, 'B': b, 'Spearman': coef, 'P-value': pval})\n",
    "\n",
    "    return pd.DataFrame.from_records(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_online(x: pd.DataFrame, y: pd.DataFrame, metric) -> pd.DataFrame:\n",
    "    selector = feature_selection.SelectKBest(similarity=metric(), k=2)\n",
    "    for xs, ys in stream.iter_pandas(x, y):\n",
    "        selector.learn_one(xs, ys)\n",
    "        \n",
    "    leaderboard = (\n",
    "        pd.DataFrame(selector.leaderboard.items(), columns=['feature', 'score'])\n",
    "          .set_index('feature')\n",
    "          .sort_values(by='score', ascending=False)\n",
    "    )\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "def features_subset_online(filename, classes, axis, label, train_size, anomaly_severity, balance, rpm_limit):\n",
    "    features = pd.read_csv(filename)\n",
    "    features = features[features['fault'].isin(classes)]\n",
    "    if rpm_limit:\n",
    "        RPM = 2900\n",
    "        RPM_RANGE = 500\n",
    "        features = features[features['rpm'].between(RPM - RPM_RANGE, RPM + RPM_RANGE, inclusive='both')]\n",
    "    \n",
    "    features = models.fault_labeling(features, classes, anomaly_severity)\n",
    "\n",
    "    groups = [\n",
    "        df.sample(frac=1, random_state=10)\n",
    "        for i, df in (\n",
    "            features.sort_values(by='severity_level').groupby('severity_level')\n",
    "        )\n",
    "    ]\n",
    "    features = pd.concat(groups).reset_index(drop=True)\n",
    "\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]]\n",
    "    y = features[label].astype('category')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, random_state=10\n",
    "    )   \n",
    "    return (\n",
    "        X_train.sort_index(), y_train.sort_index(),\n",
    "        X_test.sort_index(), y_test.sort_index()\n",
    "    )\n",
    "\n",
    "def plot_online_best_features(X, y, title, metric, n=10):\n",
    "    selector = feature_selection.SelectKBest(similarity=metric(), k=2)\n",
    "\n",
    "    best = []\n",
    "    for xs, ys in stream.iter_pandas(X, y):\n",
    "        selector.learn_one(xs, ys)\n",
    "        best.append({k: abs(v) for k, v in selector.leaderboard.items()})\n",
    "\n",
    "    # Get only n best featues to plot\n",
    "    n_top_names = [(k, abs(v)) for k, v in selector.leaderboard.items()]\n",
    "    n_top_names = sorted(n_top_names, key=lambda x: x[1], reverse=True)[:n]\n",
    "    n_top_names = set(map(lambda x: x[0], n_top_names))\n",
    "    best = [\n",
    "        {k: v for k, v in step.items() if k in n_top_names}\n",
    "        for step in best\n",
    "    ]\n",
    "\n",
    "    feature_set = pd.DataFrame.from_records(best)\n",
    "    kwargs = dict(figsize=(15, 6), grid=True, xlabel='Observation', ylabel=title)\n",
    "    if metric == selection.FisherScore:\n",
    "        kwargs['ylim'] = (0, 20) \n",
    "    feature_set.plot(**kwargs)\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "# X, y = models.features_subset(TD_FEATURES, FAULT_CLASSES, AXIS, TARGET)\n",
    "# feature_set = plot_best_features(X, y, TITLE, METRIC)\n",
    "# plt.show()\n",
    "# (feature_set.tail(1)\n",
    "#     .reset_index(drop=True)\n",
    "#     .T.rename(columns={0: TITLE})\n",
    "#     .sort_values(by=TITLE, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_effect_of_normalization(feature_set: str, axis: list, metric: Callable):\n",
    "    x, y = axis_to_magnitudes(feature_set, axis, None)\n",
    "    scores = calculate_scores(x, y, metric)\n",
    "    \n",
    "    features_normalized = selection.normalize_features(x, x.columns)\n",
    "    scores_norm = calculate_scores(features_normalized, y, metric)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    scores.head(15).plot.bar(figsize=(10, 4), grid=True, xlabel='Feature', ylabel='Metric', legend=False, title='Unnormalized', ax=ax[0])\n",
    "    scores_norm.head(15).plot.bar(figsize=(10, 4), grid=True, xlabel='Feature', ylabel='Metric', legend=False, title='Normalized', ax=ax[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def scatter_3best_features(x, y, best_features, scaler_class, target):\n",
    "    columns = best_features[:3]\n",
    "    data = x[columns].copy()\n",
    "    scaler = scaler_class()\n",
    "    data[columns] = scaler.fit_transform(data)\n",
    "    if target == 'fault':\n",
    "        models.cross_cuts_3d(data, y)\n",
    "    elif target == 'anomaly':\n",
    "        models.cross_cuts_3d_anomalies(data, y)\n",
    "\n",
    "\n",
    "def pca_feature_importance(X, n=10):\n",
    "    # Absolute values of the Eigenvectors' components corresponding to the k largest Eigenvalues.\n",
    "    model = PCA(n_components=3).fit(X)\n",
    "    X_pc = model.transform(X)\n",
    "\n",
    "    columns = list(X.columns)\n",
    "    percentages = [(100 * (np.flip(np.sort(np.abs(pc))) / np.sum(np.abs(pc))))[:n] for pc in model.components_]\n",
    "    most_important = [np.flip(np.argsort(np.abs(pc)))[:n] for pc in model.components_]\n",
    "\n",
    "    for i, pc in enumerate(most_important):\n",
    "        print(f'PC{i+1} ({model.explained_variance_ratio_[i] * 100:.4f} %)')\n",
    "        print([columns[x] for x in pc])\n",
    "        print(percentages[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show effect of normalization on metrics\n",
    "Result: normalization has no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145     False\n",
      "146     False\n",
      "147     False\n",
      "148     False\n",
      "149     False\n",
      "        ...  \n",
      "9730     True\n",
      "9731     True\n",
      "9732     True\n",
      "9733     True\n",
      "9734     True\n",
      "Name: anomaly, Length: 1500, dtype: category\n",
      "Categories (2, bool): [False, True]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_effect_of_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTD_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPLACE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMETRICS_OFFLINE\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mshow_effect_of_normalization\u001b[0;34m(feature_set, axis, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_effect_of_normalization\u001b[39m(feature_set: \u001b[38;5;28mstr\u001b[39m, axis: \u001b[38;5;28mlist\u001b[39m, metric: Callable):\n\u001b[0;32m----> 2\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43maxis_to_magnitudes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     scores \u001b[38;5;241m=\u001b[39m calculate_scores(x, y, metric)\n\u001b[1;32m      5\u001b[0m     features_normalized \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mnormalize_features(x, x\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36maxis_to_magnitudes\u001b[0;34m(feature_set, axis, window_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maxis_to_magnitudes\u001b[39m(feature_set: \u001b[38;5;28mstr\u001b[39m, axis: \u001b[38;5;28mlist\u001b[39m, window_size: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     x, y, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures_subset_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mFAULT_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mVAR_TARGET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAIN_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mANOMALY_SEVERITY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBALANCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mRPM_LIMIT\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(window_size, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     51\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mloc[:,x\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mfeatures_subset_offline\u001b[0;34m(filename, classes, axis, label, train_size, anomaly_severity, balance, rpm_limit)\u001b[0m\n\u001b[1;32m     30\u001b[0m     oversample \u001b[38;5;241m=\u001b[39m RandomOverSampler(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot majority\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     31\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m oversample\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[0;32m---> 33\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_test, y_test\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2670\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2229\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m \n\u001b[1;32m   2198\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2229\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dp/lib/python3.11/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_imbalance(feature_set, axis):\n",
    "    _, y = axis_to_magnitudes(feature_set, axis, None)\n",
    "    counter = Counter(y)\n",
    "    for k, v in counter.items():\n",
    "        per = v / len(y) * 100\n",
    "        print(f'Class={k}, n={v} ({per:.3f}%%)')\n",
    "    \n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "    plt.show()\n",
    "\n",
    "plot_class_imbalance(TD_FEATURES, PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_imbalance(FD_FEATURES, PLACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature set #1: Custom features time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = corr_among_features(TD_FEATURES, PLACE)\n",
    "c.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = feature_ranking(TD_FEATURES)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = axis_to_magnitudes(TD_FEATURES, PLACE, None)\n",
    "scatter_3best_features(x, y, list(ranks.index), MinMaxScaler, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = axis_to_magnitudes(TD_FEATURES, PLACE, None)\n",
    "scatter_3best_features(x, y, list(ranks.index), StandardScaler, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feature_importance(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature set #2: Custom features frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = corr_among_features(FD_FEATURES, PLACE)\n",
    "c.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = feature_ranking(FD_FEATURES)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain: window size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = (2**6, 2**8, 2**10, 2**12, 2**14)\n",
    "\n",
    "win_len = window_sizes[0]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Window size:', win_len)\n",
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_among_features(FD_FEATURES, PLACE, win_len).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[1]\n",
    "print('Window size:', win_len)\n",
    "metric_ranks = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_among_features(FD_FEATURES, PLACE, win_len).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[2]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[3]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain: window size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[4]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of ranks depending in window size (scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_window_ranks = pd.DataFrame()\n",
    "for win in window_sizes:\n",
    "    scores = feature_ranking(FD_FEATURES, win)\n",
    "    ranks = ensemble_feature_ranking(scores)\n",
    "    feature_window_ranks[win] = ranks\n",
    "\n",
    "feature_window_ranks.sort_values(by=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
