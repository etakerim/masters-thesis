{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch features\n",
    "\n",
    "Exports (RPM limit = False / True):\n",
    "```\n",
    "    - SHAFT, A, fault\n",
    "    - SHAFT, B, fault\n",
    "    - BEARINGS, A, fault\n",
    "    - BEARINGS, B, fault\n",
    "\n",
    "    - SHAFT, A, anomaly, 0.6\n",
    "    - SHAFT, B, anomaly, 0.6\n",
    "    - BEARINGS, A, anomaly, 0.6\n",
    "    - BEARINGS, B, anomaly, 0.6\n",
    "\n",
    "    - SHAFT, A, anomaly, 0.9\n",
    "    - SHAFT, B, anomaly, 0.9\n",
    "    - BEARINGS, A, anomaly, 0.9\n",
    "    - BEARINGS, B, anomaly, 0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEMENTS = [['ax', 'ay', 'az'], ['bx', 'by', 'bz']]\n",
    "TARGETS = ['fault', 'anomaly']\n",
    "ANOMALY_SEVERITIES = [0.6, 0.9]\n",
    "SHAFT_FAULTS = {'normal': 'N', 'imbalance': 'I', 'horizontal-misalignment': 'HM', 'vertical-misalignment': 'VM'}\n",
    "BEARING_FAULTS = {'overhang-cage_fault': 'O-Cage', 'underhang-cage_fault': 'U-Cage',\n",
    "                  'underhang-ball_fault': 'U-Ball', 'overhang-ball_fault': 'O-Ball',\n",
    "                  'underhang-outer_race': 'U-Race', 'overhang-ball_fault': 'O-Race'}\n",
    "FAULT_TYPES = [SHAFT_FAULTS, BEARING_FAULTS]\n",
    "\n",
    "\n",
    "CORR_SIGNIFICANT = 0.95\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "\n",
    "# Variables\n",
    "PLACE = PLACEMENTS[1]\n",
    "VAR_TARGET = TARGETS[1]\n",
    "ANOMALY_SEVERITY = ANOMALY_SEVERITIES[1]       # If it is anomaly\n",
    "\n",
    "FAULT_CLASSES = FAULT_TYPES[1]\n",
    "RPM_LIMIT = False                         # False, True\n",
    "BALANCE = True                           # False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from vibrodiagnostics import selection, discovery, models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean, spearmanr, kendalltau\n",
    "\n",
    "from typing import Callable, Dict, Tuple, List\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from river import feature_selection, stream, preprocessing\n",
    "\n",
    "\n",
    "PATH_PREFIX = '../../datasets/'\n",
    "FEATURES_PATH =  os.path.join(PATH_PREFIX, 'features_data')\n",
    "\n",
    "TD_FD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_AND_FREQ_FEATURES_PATH)\n",
    "TD_FEATURES = os.path.join(FEATURES_PATH, selection.TIME_FEATURES_PATH)\n",
    "FD_FEATURES = os.path.join(FEATURES_PATH, selection.FREQ_FEATURES_PATH)\n",
    "\n",
    "METRICS_TITLES = ('Correlation', 'F statistic', 'Mutual information')\n",
    "METRICS_ONLINE = (selection.Correlation, selection.FisherScore, selection.MutualInformation)\n",
    "METRICS_OFFLINE = (selection.corr_classif, f_classif, mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(x: pd.DataFrame, y: pd.DataFrame, metric: Callable) -> pd.DataFrame:\n",
    "    scores = metric(x, y)\n",
    "    if isinstance(scores, tuple):\n",
    "        scores = scores[0]\n",
    "    leaderboard = (\n",
    "        pd.DataFrame(zip(x.columns, scores), columns=['feature', 'score'])\n",
    "        .set_index('feature')\n",
    "        .sort_values(by='score', ascending=False)\n",
    "    )\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "def features_subset_offline(\n",
    "        filename: str, \n",
    "        classes: Dict[str, str],\n",
    "        axis: Tuple[str], \n",
    "        label: str,\n",
    "        train_size: float, \n",
    "        anomaly_severity: float,\n",
    "        balance: bool,\n",
    "        rpm_limit: bool\n",
    "    ):\n",
    "    features = pd.read_csv(filename).fillna(0)\n",
    "    features = features[features['fault'].isin(classes)]\n",
    "    if rpm_limit:\n",
    "        RPM = 2900\n",
    "        RPM_RANGE = 500\n",
    "        features = features[features['rpm'].between(RPM - RPM_RANGE, RPM + RPM_RANGE, inclusive='both')]\n",
    "    \n",
    "    features = models.fault_labeling(features, classes, anomaly_severity)\n",
    "\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]].fillna(0)\n",
    "    y = features[label].astype('category')\n",
    "\n",
    "    # Balance dataset\n",
    "    if balance:\n",
    "        oversample = RandomOverSampler(sampling_strategy='not majority', random_state=10)\n",
    "        X, y = oversample.fit_resample(X, y.to_numpy())\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, stratify=y, random_state=10\n",
    "    )\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def axis_to_magnitude(x: pd.DataFrame, axis: list, window_size: int):\n",
    "    if isinstance(window_size, int):\n",
    "        x = x.loc[:,x.columns.str.endswith(f'_{window_size}')]\n",
    "        x.columns = x.columns.str.extract(r'([\\w\\_]+)_(\\w+)$')[0]\n",
    "\n",
    "    feature_names = x.columns.str.extract(r'([a-z]{2})_([\\w\\_\\-]+)')[1].unique()\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "    for name in feature_names:\n",
    "        vector_dims = [f'{dim}_{name}' for dim in axis]\n",
    "        result[name] = x[vector_dims].apply(np.linalg.norm, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_feature_set(feature_set: str, axis: list, window_size: int, online=False) -> tuple:\n",
    "    if online:\n",
    "        func = features_subset_online\n",
    "    else:\n",
    "        func = features_subset_offline\n",
    "\n",
    "    x_train, y_train, x_test, y_test = func(\n",
    "        feature_set,\n",
    "        FAULT_CLASSES,\n",
    "        axis,\n",
    "        VAR_TARGET,\n",
    "        TRAIN_SIZE,\n",
    "        ANOMALY_SEVERITY,\n",
    "        BALANCE,\n",
    "        RPM_LIMIT\n",
    "    )\n",
    "    x_train_mag = axis_to_magnitude(x_train, axis, window_size)\n",
    "    x_test_mag = axis_to_magnitude(x_test, axis, window_size)\n",
    "\n",
    "    return x_train_mag, y_train, x_test_mag, y_test\n",
    "\n",
    "\n",
    "def feature_ranking(feature_set: str, window_size=None):\n",
    "    axis = PLACE\n",
    "    metric_ranks = pd.DataFrame()\n",
    "\n",
    "    for metric_name, metric in zip(('corr', 'f_stat', 'mi'), METRICS_OFFLINE):\n",
    "        x, y, _, _ = load_feature_set(feature_set, axis, window_size)\n",
    "        scores = calculate_scores(x, y, metric)\n",
    "        metric_ranks[metric_name] = scores\n",
    "\n",
    "    return metric_ranks\n",
    "\n",
    "\n",
    "def corr_among_features(feature_set, axis, window_size=None):\n",
    "    x, y, _, _ = load_feature_set(feature_set, axis, window_size)\n",
    "\n",
    "    correlations = [\n",
    "        {'feature_1': k[0], 'feature_2': k[1], 'corr': v}\n",
    "        for k, v in x.corr().abs().stack().to_dict().items()\n",
    "        if k[0] != k[1]\n",
    "    ]\n",
    "    correlations = pd.DataFrame.from_records(correlations).sort_values(by='corr', ascending=False)\n",
    "    correlations[correlations['corr'] > 0.7]\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def extract_best_features(x_train, y_train, best_features, corr, n=3) -> List[str]:\n",
    "    # Remove correlated features independent of tuple order\n",
    "    correlations = corr[corr['corr'] >= CORR_SIGNIFICANT][\n",
    "        ['feature_1', 'feature_2']\n",
    "    ].to_numpy()\n",
    "    similar_pairs = set([(a, b) for a, b in correlations])\n",
    "    similar_pairs.update([(b, a) for a, b in correlations])\n",
    "\n",
    "    columns = []\n",
    "    for feature in best_features:\n",
    "        # Make pairs with existing columns\n",
    "        candidates = [\n",
    "            col for col in columns \n",
    "            if (feature, col) in similar_pairs\n",
    "        ]        \n",
    "        # Append only if not correlation detected\n",
    "        if len(candidates) == 0:\n",
    "            columns.append(feature)\n",
    "\n",
    "    # Limit to 3 features\n",
    "    columns = columns[:n]\n",
    "    print('Best features:', columns)\n",
    "    return columns\n",
    "\n",
    "\n",
    "def plot_scores(metric_ranks, n=None):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    for i, col in enumerate(('corr', 'f_stat', 'mi')):\n",
    "        scores = metric_ranks[col].sort_values(ascending=False)\n",
    "        if n is not None:\n",
    "            scores = scores.iloc[:n]\n",
    "        ax[i].bar(scores.index, scores)\n",
    "\n",
    "    for i, col_name in enumerate(('Correlation', 'F statistic', 'Mutual information')):\n",
    "        ax[i].set_xticks(ax[i].get_xticks())\n",
    "        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        ax[i].grid()\n",
    "        ax[i].set_xlabel('Feature')\n",
    "        ax[i].set_ylabel(col_name)\n",
    "\n",
    "\n",
    "def ensemble_feature_ranking(scores: pd.DataFrame):\n",
    "    ranks = scores.rank(axis='rows', method='first', ascending=False)\n",
    "    return ranks.apply(gmean, axis=1).sort_values().to_frame(name='rank') # Rank product\n",
    "\n",
    "\n",
    "def rank_correlation(scores: pd.DataFrame):\n",
    "    # spearman's rho vs kendall's tau\n",
    "    ranks = scores.rank(axis='rows', method='first', ascending=False)\n",
    "    correlations = []\n",
    "    for a, b in itertools.combinations(ranks.columns, r=2):\n",
    "        coef, pval = spearmanr(ranks[a], ranks[b])\n",
    "        # coef, pval = kendalltau(ranks[a], ranks[b])\n",
    "        correlations.append({'A': a, 'B': b, 'Spearman': coef, 'P-value': pval})\n",
    "\n",
    "    return pd.DataFrame.from_records(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_online(x: pd.DataFrame, y: pd.DataFrame, metric) -> pd.DataFrame:\n",
    "    selector = feature_selection.SelectKBest(similarity=metric(), k=2)\n",
    "    for xs, ys in stream.iter_pandas(x, y):\n",
    "        selector.learn_one(xs, ys)\n",
    "        \n",
    "    leaderboard = (\n",
    "        pd.DataFrame(selector.leaderboard.items(), columns=['feature', 'score'])\n",
    "          .set_index('feature')\n",
    "          .sort_values(by='score', ascending=False)\n",
    "    )\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "def features_subset_online(\n",
    "        filename: str, \n",
    "        classes: Dict[str, str],\n",
    "        axis: Tuple[str], \n",
    "        label: str,\n",
    "        train_size: float, \n",
    "        anomaly_severity: float,\n",
    "        balance: bool,\n",
    "        rpm_limit: bool\n",
    "    ):\n",
    "    features = pd.read_csv(filename)\n",
    "    features = features[features['fault'].isin(classes)]\n",
    "    if rpm_limit:\n",
    "        RPM = 2900\n",
    "        RPM_RANGE = 500\n",
    "        features = features[features['rpm'].between(RPM - RPM_RANGE, RPM + RPM_RANGE, inclusive='both')]\n",
    "    \n",
    "    features = models.fault_labeling(features, classes, anomaly_severity)\n",
    "\n",
    "    groups = [\n",
    "        df.sample(frac=1, random_state=10)\n",
    "        for i, df in (\n",
    "            features.sort_values(by='severity_level').groupby('severity_level')\n",
    "        )\n",
    "    ]\n",
    "    features = pd.concat(groups).reset_index(drop=True)\n",
    "\n",
    "    columns = features.columns.str.startswith(tuple(axis))\n",
    "    X = features[features.columns[columns]].fillna(0)\n",
    "    y = features[label].astype('category')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, random_state=10\n",
    "    )   \n",
    "    return (\n",
    "        X_train.sort_index(), y_train.sort_index(),\n",
    "        X_test.sort_index(), y_test.sort_index()\n",
    "    )\n",
    "\n",
    "def plot_online_best_features(x: pd.DataFrame, y: pd.Series, title: str, metric: Callable) -> pd.DataFrame:\n",
    "    selector = feature_selection.SelectKBest(similarity=metric(), k=2)\n",
    "\n",
    "    best = []\n",
    "    for xs, ys in stream.iter_pandas(x, y):\n",
    "        selector.learn_one(xs, ys)\n",
    "        best.append({k: abs(v) for k, v in selector.leaderboard.items()})\n",
    "\n",
    "    # Get only n best featues to plot\n",
    "    n = 15\n",
    "    n_top_names = [(k, abs(v)) for k, v in selector.leaderboard.items()]\n",
    "    n_top_names = sorted(n_top_names, key=lambda x: x[1], reverse=True)[:n]\n",
    "    n_top_names = set(map(lambda x: x[0], n_top_names))\n",
    "    best = [\n",
    "        {k: v for k, v in step.items() if k in n_top_names}\n",
    "        for step in best\n",
    "    ]\n",
    "\n",
    "    feature_set = pd.DataFrame.from_records(best)\n",
    "    kwargs = dict(figsize=(15, 6), grid=True, xlabel='Observation', ylabel=title)\n",
    "    if metric == selection.FisherScore:\n",
    "        kwargs['ylim'] = (0, 2 * feature_set.describe().max(axis=1)['75%']) \n",
    "    feature_set.plot(**kwargs)\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def plot_online_best_ensemble_features(x: pd.DataFrame, y: pd.Series, title: str) -> pd.DataFrame:\n",
    "    estimators = [\n",
    "        feature_selection.SelectKBest(similarity=metric(), k=2)\n",
    "        for metric in METRICS_ONLINE\n",
    "    ]\n",
    "    rating = preprocessing.MinMaxScaler()\n",
    "\n",
    "    best = []\n",
    "    for xs, ys in stream.iter_pandas(x, y):\n",
    "        for method in estimators:\n",
    "            method.learn_one(xs, ys)\n",
    "\n",
    "        # TODO: Remove correlated features\n",
    "\n",
    "        scores = [method.leaderboard.copy() for method in estimators]\n",
    "        scores = pd.DataFrame.from_records(scores).T\n",
    "        ranks = scores.rank(axis='rows', method='first', ascending=False)\n",
    "        ranks = ranks.apply(gmean, axis=1).to_dict()   # Smallest rank is the best\n",
    "\n",
    "        # Scale ranks to interval (0, 1) - worst to best\n",
    "        values = [{'x': x} for x in ranks.values()]\n",
    "        for value in values:\n",
    "            rating.learn_one(value)\n",
    "\n",
    "        ranks = {k: 1 - rating.transform_one({'x': v})['x'] for k, v in ranks.items()}\n",
    "        best.append(ranks)   \n",
    "\n",
    "    # Get only n best featues to plot\n",
    "    n = 15\n",
    "    n_top_names = [(k, abs(v)) for k, v in best[-1].items()]\n",
    "    n_top_names = sorted(n_top_names, key=lambda x: x[1], reverse=True)[:n]\n",
    "    n_top_names = set(map(lambda x: x[0], n_top_names))\n",
    "    best = [\n",
    "        {k: v for k, v in step.items() if k in n_top_names}\n",
    "        for step in best\n",
    "    ]                       \n",
    "\n",
    "    feature_set = pd.DataFrame.from_records(best)\n",
    "    feature_set.plot(figsize=(15, 6), grid=True, xlabel='Observation', ylabel=title)\n",
    "\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_effect_of_normalization(feature_set: str, axis: list, metric: Callable):\n",
    "    x, y, _, _ = load_feature_set(feature_set, axis, None)\n",
    "    scores = calculate_scores(x, y, metric)\n",
    "    \n",
    "    features_normalized = selection.normalize_features(x, x.columns)\n",
    "    scores_norm = calculate_scores(features_normalized, y, metric)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    scores.head(15).plot.bar(figsize=(10, 4), grid=True, xlabel='Feature', ylabel='Metric', legend=False, title='Unnormalized', ax=ax[0])\n",
    "    scores_norm.head(15).plot.bar(figsize=(10, 4), grid=True, xlabel='Feature', ylabel='Metric', legend=False, title='Normalized', ax=ax[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def scatter_features(x_train, y_train, x_test, y_test, columns, target):\n",
    "    train_data = x_train[columns].copy()\n",
    "    test_data = x_test[columns].copy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data[columns] = scaler.fit_transform(train_data)\n",
    "    test_data[columns] = scaler.transform(test_data)\n",
    "\n",
    "    print('[Train] Silhouette coefficient: ', silhouette_score(train_data, y_train))\n",
    "    print('[Test] Silhouette coefficient: ', silhouette_score(test_data, y_test))\n",
    "\n",
    "    if target == 'fault':\n",
    "        models.cross_cuts_3d(train_data, y_train)\n",
    "        plt.show()\n",
    "        models.cross_cuts_3d(test_data, y_test)\n",
    "        plt.show()\n",
    "\n",
    "    elif target == 'anomaly':\n",
    "        models.cross_cuts_3d_anomalies(train_data, y_train)\n",
    "        plt.show()\n",
    "        models.cross_cuts_3d_anomalies(test_data, y_test)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def pca_scatter(x_train, y_train, x_test, y_test, target):\n",
    "    y_train = y_train.reset_index(drop=True).astype('category')\n",
    "    y_test = y_test.reset_index(drop=True).astype('category')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "    x_test[x_test.columns] = scaler.transform(x_test)\n",
    "\n",
    "    model = PCA(n_components=3).fit(x_train)\n",
    "    x_train = pd.DataFrame(model.transform(x_train))\n",
    "    x_test = pd.DataFrame(model.transform(x_test))\n",
    "\n",
    "    print('[PCA Train] Silhouette coefficient: ', silhouette_score(x_train, y_train))\n",
    "    print('[PCA Test] Silhouette coefficient: ', silhouette_score(x_test, y_test))\n",
    "    \n",
    "    print('PC explained variance:', model.explained_variance_ratio_)\n",
    "    if target == 'fault':\n",
    "        models.cross_cuts_3d(x_train, y_train)\n",
    "        plt.show()\n",
    "        models.cross_cuts_3d(x_test, y_test)\n",
    "        plt.show()\n",
    "\n",
    "    elif target == 'anomaly':\n",
    "        models.cross_cuts_3d_anomalies(x_train, y_train)\n",
    "        plt.show()\n",
    "        models.cross_cuts_3d_anomalies(x_test, y_test)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def pca_feature_importance(X, n=10):\n",
    "    # Absolute values of the Eigenvectors' components corresponding to the k largest Eigenvalues.\n",
    "    model = PCA(n_components=3).fit(X)\n",
    "    X_pc = model.transform(X)\n",
    "\n",
    "    columns = list(X.columns)\n",
    "    percentages = [(100 * (np.flip(np.sort(np.abs(pc))) / np.sum(np.abs(pc))))[:n] for pc in model.components_]\n",
    "    most_important = [np.flip(np.argsort(np.abs(pc)))[:n] for pc in model.components_]\n",
    "\n",
    "    for i, pc in enumerate(most_important):\n",
    "        print(f'PC{i+1} ({model.explained_variance_ratio_[i] * 100:.4f} %)')\n",
    "        print([columns[x] for x in pc])\n",
    "        print(percentages[i])\n",
    "\n",
    "\n",
    "def plot_label_occurences(y):\n",
    "    observations = []\n",
    "    y = y.astype('category')\n",
    "    columns = list(y.cat.categories)\n",
    "    empty = dict(zip(columns, len(columns) * [0]))\n",
    "\n",
    "    for row in y:\n",
    "        sample = empty.copy()\n",
    "        sample[row] = 1\n",
    "        observations.append(sample)\n",
    "\n",
    "    class_occurences = pd.DataFrame.from_records(observations).cumsum()\n",
    "    class_occurences.plot(grid=True, figsize=(10, 3), xlabel='Observations', ylabel='Label occurences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _, _ = load_feature_set(TD_FEATURES, PLACE, None)\n",
    "ax = x.boxplot(figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x[x.columns] = scaler.fit_transform(x)\n",
    "ax = x.boxplot(figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "x[x.columns] = scaler.fit_transform(x)\n",
    "ax = x.boxplot(figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, 2**14)\n",
    "ax = x.boxplot(figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x[x.columns] = scaler.fit_transform(x)\n",
    "ax = x.boxplot(figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = x.plot.hist(figsize=(10, 20), subplots=True, bins=100, grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show effect of normalization on metrics\n",
    "Result: normalization has no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_effect_of_normalization(TD_FEATURES, PLACE, METRICS_OFFLINE[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_imbalance(feature_set, axis):\n",
    "    _, y, _, _ = load_feature_set(feature_set, axis, None)\n",
    "    counter = Counter(y)\n",
    "    for k, v in counter.items():\n",
    "        per = v / len(y) * 100\n",
    "        print(f'Class={k}, n={v} ({per:.3f}%%)')\n",
    "    \n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "    plt.show()\n",
    "\n",
    "plot_class_imbalance(TD_FEATURES, PLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_imbalance(FD_FEATURES, PLACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature set #1: Temporal domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(TD_FEATURES, PLACE)\n",
    "corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = feature_ranking(TD_FEATURES)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(TD_FEATURES, PLACE, None)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature selection to PCA\n",
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feature_importance(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(TD_FEATURES, PLACE, None, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (Correlation)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature set #2: Spectral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = corr_among_features(FD_FEATURES, PLACE)\n",
    "c.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = feature_ranking(FD_FEATURES)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, None)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, None, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (Correlation)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain: window size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = (2**6, 2**8, 2**10, 2**12, 2**14)\n",
    "\n",
    "win_len = window_sizes[0]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Window size:', win_len)\n",
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(FD_FEATURES, PLACE, win_len)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, win_len)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, None, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[1]\n",
    "print('Window size:', win_len)\n",
    "metric_ranks = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(FD_FEATURES, PLACE, win_len)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, win_len)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, win_len, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[2]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(FD_FEATURES, PLACE, win_len)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, win_len)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, win_len, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Frequency domain: window size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[3]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(FD_FEATURES, PLACE, win_len)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, win_len)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, win_len, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain: window size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = window_sizes[4]\n",
    "print('Window size:', win_len)\n",
    "metric_scores = feature_ranking(FD_FEATURES, win_len)\n",
    "metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ensemble_feature_ranking(metric_scores)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_among_features(FD_FEATURES, PLACE, win_len)\n",
    "corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_feature_set(FD_FEATURES, PLACE, win_len)\n",
    "columns = extract_best_features(x_train, y_train, list(ranks.index), corr)\n",
    "scatter_features(x_train, y_train, x_test, y_test, columns, VAR_TARGET)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(x_train, y_train, x_test, y_test, VAR_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning\n",
    "x, y, _, _ = load_feature_set(FD_FEATURES, PLACE, win_len, online=True)\n",
    "plot_label_occurences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[0], METRICS_ONLINE[0])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[0]})\n",
    "    .sort_values(by=METRICS_TITLES[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning (F statistic)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[1], METRICS_ONLINE[1])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[1]})\n",
    "    .sort_values(by=METRICS_TITLES[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Online learning (Mutual information)\n",
    "online_feature_set = plot_online_best_features(x, y, METRICS_TITLES[2], METRICS_ONLINE[2])\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: METRICS_TITLES[2]})\n",
    "    .sort_values(by=METRICS_TITLES[2], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Rank product ensemble'\n",
    "online_feature_set = plot_online_best_ensemble_features(x, y, title)\n",
    "plt.show()\n",
    "\n",
    "(online_feature_set.tail(1)\n",
    "    .reset_index(drop=True)\n",
    "    .T.rename(columns={0: title})\n",
    "    .sort_values(by=title, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of ranks depending in window size (scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_window_ranks = pd.DataFrame()\n",
    "for win in window_sizes:\n",
    "    scores = feature_ranking(FD_FEATURES, win)\n",
    "    ranks = ensemble_feature_ranking(scores)\n",
    "    feature_window_ranks[win] = ranks\n",
    "\n",
    "feature_window_ranks.sort_values(by=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
